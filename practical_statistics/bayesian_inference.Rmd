---
title: "Notes on practical statistics IV"
output:
  html_document:
    toc: true
    fig_caption: true
    number_sections: true
---
<h1>An Introduction to Bayesian Inference</h1>

One Mode of Motivation: we have seen, when estimating parameters, that we can retrieve mean estimates and confidence intervals. But can we get the distribution? One method to do so is Bayesian paramater estimation model fitting.

#Bayes' Theorem

$$P(A|B) = \frac{P(B|A)P(A)}{P(B)}$$

Easy to prove: so do it.

#Bayes' Theorem and Parameter Estimation

##Priors, likelihoods, posteriors and parameter estimation

**Example**: biased coin flip with binomial probability $P(H)=\lambda$ and data $D$ consisting of $k$ heads and $n-k$ tails.

**Challenge**: Given the data $D$, we want to estimate the parameter $\lambda$.

**Method**: Using Bayes' Theorem, we see that

$$P(\lambda | D) = \frac{P(D | \lambda)P(\lambda)}{P(D)} \propto P( D | \lambda)P(\lambda).$$

In this equation, we call $P(\lambda)$ the *prior* (distribution), $P(D|\lambda)$ the *likelihood* (we've seen this before in MLE) and $P(\lambda | D)$ the *posterior* (distribution). The intuition behind the nomenclature is as follows: the *prior* is the distribution containing our knowledge about $\lambda$ prior to the introduction $D$ & the *posterior* is the distribution containing our knowledge about $\lambda$ after considering the data $D$.

**Note**: What is the prior? Really, what do we know about $\lambda$ before we see any data? Well, as it is a probability, we know that $0 \leq \lambda \leq 1$. If we haven't flipped any coins yet, we don't know much else: so it seems logical that all values of $\lambda$ are equally like, i.e., $P(\lambda) = 1,$ for $0 \leq \lambda \leq 1$. This is known as an *uninformative prior* because it contains little information (there are other uninformative priors we may use in this situation, such as the *Jeffreys prior*, to be discussed later). People who like to hate on Bayesian inference tend to claim that the need to choose a prior makes Bayesian methods somewhat arbitrary, but as we'll now see, if you have enough data, the likelihood dominates over the prior and the latter doesn't matter so much.

**Example**: Binomial -- provide derivation and figures below are for $n=100,k=40$ & $n=1000,k=450$, respectively.

```{r , fig.width = 8 , fig.height = 3 , message = FALSE , echo=FALSE}
#see here: http://www.r-bloggers.com/r-and-bayesian-statistics/
library( ggplot2 )
require(gridExtra)
n1 <- 100
k1 <- 40
lambda <- seq( 0 , 1 , by = 0.01)
prior <- rep( 1 , length(lambda))
likelihood1 <- lambda**k1*(1-lambda)**(n1-k1)
posterior1 <- likelihood1*prior
plot1 <- qplot( lambda , posterior1, geom = "line" ) 
##
n2 <- 1000
k2 <- 450
likelihood2 <- lambda**k2*(1-lambda)**(n2-k2)
posterior2 <- likelihood2*prior
plot2 <- qplot( lambda , posterior2, geom = "line" ) 
grid.arrange(plot1, plot2, ncol=2)
```

**Exercise 1**:

Compute and plot the posterior for two priors (uniform & 'crazy coin') for a variety of $n = 1 , 4 , 16 , 64, 256 , 1024 , 4096$.

**What to report once you have the posterior distribution**:

- The **posterior mode** (note that, in the case of a uniform prior, this is precisely the **maximum likelihood estimate**);
- The **standard deviation of the posterior** if it is approximately Gaussian around the mode;
- **$95\%$ confidence intervals** otherwise;
- Report **the distribution**!

**Slight tangent**: Now we see that the MLE we looked at in detail in Workshop III is a special case of Bayesian inference, we can view a number of our established techniques as a nested sequence:

$$\text{linear regression} \subset \text{nonlinear least squares} \subset \text{maximum likelihood estimation} \subset \text{Bayesian inference}$$

##Alternative priors

There is a subtle issue in choosing the uniform prior on the *binomial probability* $\lambda$. The issue is as follows: let's say that 

1. I choose the *uniform prior* on $\lambda$, claiming that I do so because I have *absolutely no prior knowledge* as to what $\lambda$ is;
2. Olivier Trottier (teaching assistant) is looking at the same data as me BUT Oli is thinking about the scientific question in terms of the *odds parameter* $\tau = \frac{\lambda}{1 - \lambda}$; Oli rightly feels that he has no *prior knowledge* as to what this $\tau$ is and thus chooses the uniform prior on $\tau$.

With a bit of algebra (transformation of variables), we can show that choosing the *uniform prior* on $\lambda$ amounts to choosing a decidedly *non-uniform* prior on $\tau$. So Oli and I have actually chosen different priors, using the same philosophy. How do we avoid this happening? Enter the **Jeffreys prior**.

<h4>The Jeffreys prior</h4>

The *Jeffreys prior* is invariant, in following sense: if I choose the Jeffreys prior on a set of variables and Oli chooses the Jeffreys prior on a transformation (reparametrization) of those variables, then we have both chosen the same prior.

We include the definition of the Jeffreys prior in the one-parameter case, for completeness. You will need to know a bit more about probability theory to understand the definition, however. If this is not the case, skip it or learn it!

**Definition**:

For the one-parameter case, the **Jeffreys prior** is defined to be

$$p(\theta) = \sqrt{I(\theta)},$$

where $I$ is the Fisher Information. See [here](https://eventuallyalmosteverywhere.wordpress.com/2013/05/10/bayesian-inference-and-the-jeffreys-prior/) & the Gelman book for further details.

**Example**:

In the case of the binomial distribution, the Jeffreys prior for $\lambda$ is

$$p(\lambda) = \frac{1}{\sqrt{\lambda(1-\lambda)}}.$$

**Intuition**: You should try to use the Jeffreys prior when there are multiple ways/parametrizations to describe the distribution in the likelihood function.

**Example**: For the exponential distribution $P(t) = \mu\text{exp}(-\mu t)$, we can paramterize the distribution using the characteristic rate $\mu$ OR the characteristic time $1/\mu$. The Jeffreys prior on $\mu$ is 

$$P(\mu) = \mu^{-1}.$$

**Example**: For the gamma distribution $P(x|\alpha,\beta) = \frac{\beta^{\alpha}}{\Gamma(\alpha)}x^{\alpha - 1}e^{-\beta x}$, the Jeffreys prior is

$$P(\alpha , \beta) = (\alpha\beta)^{-1}.$$

#Computing the posterior mode & the standard deviation

##Finding the best estimate: the posterior mode

Recall that we need to find the *posterior mode*. Just as we saw in MLE, the numbers in question get super-small super-quickly so we actually minimize the *negative log-posterior*. As

$$P(\lambda | D)  \propto P( D | \lambda)P(\lambda),$$

the *negative log-posterior* is

$$-\text{ln}(P(\lambda | D)) = C + \sum\text{ln}(P( D_i | \lambda)) + \text{ln}(P(\lambda)).$$

To minimize this with respect to $\lambda$, we can forget about the constant $C$.


**Example** (uniform prior):
```{r , fig.width = 4 , fig.height = 3}
bin_data <- rbinom(1024, 1  , 0.75)
Lp <- function( p ){
  R <- dbinom( bin_data , 1, p ) #binomial function w/ probability p
  -sum(log(R)) #-ve log likelihood
}
res <- optim(c(0.5) , Lp , method = "L-BFGS-B" , lower = 0.0001 , upper = 0.999 ) #minimize -LL with boundary = 0
res$par
```

**Example** (Jeffreys prior):
```{r , fig.width = 4 , fig.height = 3}
bin_data <- rbinom(1024, 1  , 0.25)
Lp <- function( p ){
  R <- dbinom( bin_data , 1, p ) #binomial function w/ probability p
  -sum(log(R)) -log(1/sqrt(p*(1-p)))#-ve log likelihood
}
res <- optim(c(0.5) , Lp , method = "L-BFGS-B" , lower = 0.0001 , upper = 0.999 ) #minimize -LL with boundary = 0
res$par
```

**What about error bars in the Bayesian setting?** We'll check out how to compute them after a couple more examples.

###Bayesian parameter estimation for an exponential distribution

We're now going to check back in with our microtubule lifetime data from (Gardner et al., 2011) to perform parameter estimation for i) an exponential distribution & ii) a gamma distribution.

**Exercise**: Below find the code the compute the *posterior mode* of the rate parameter of an exponential distribution, given the microtubule lifetime data & a uniform prior. Hack the code in the required line to use the Jeffreys prior.

```{r , fig.width = 8 , fig.height = 3 , message = FALSE , echo=TRUE}
data <- read.csv("MG_catastrophe_data/lifetimes_tubulin.csv" , check.names=FALSE) #load data from #.csv
x <- data$`12_uM`
Lp_exp <- function( rate ){
  R <- dexp( x , rate , log = TRUE)
  -sum(R) #add term here for Jeffrey's prior
}
#fit_exp <- mle( Lp_exp , start = list( rate = 1) )
#summary(fit_exp)
res_exp <- optim(c(1.5) , Lp_exp, method = "L-BFGS-B" , lower = 0.000001 ) #minimize -LL with boundary = 0
res_exp$par
```

**Solution**:
```{r , fig.width = 8 , fig.height = 3 , message = FALSE , echo=TRUE}
data <- read.csv("MG_catastrophe_data/lifetimes_tubulin.csv" , check.names=FALSE) #load data from #.csv
x <- data$`12_uM`
Lp_exp <- function( rate ){
  R <- dexp( x , rate , log = TRUE)
  -sum(R) - log( rate )
}
#fit_exp <- mle( Lp_exp , start = list( rate = 1) )
#summary(fit_exp)
res_exp <- optim(c(1.5) , Lp_exp, method = "L-BFGS-B" , lower = 0.000001 ) #minimize -LL with boundary = 0
res_exp$par
```

**What's happening in the background here**:

Exponential distribution $P(x) = \frac{\text{exp}(-x/\mu)}{\mu}$. Assume uniform prior on $\mu$ (we will do the same using Jeffreys' prior), the posterior

$$P(\mu|D) \propto \prod_i \frac{\text{exp}(-x_i/\mu)}{\mu}$$

and the log-posterior $\text{log}P(\mu|D)= C + n\text{log}(1/\mu) - \frac{\sum x_i}{\mu}.$ So we wish to minimize the *negative log posterior*.
```{r , fig.width = 8 , fig.height = 3 , echo=FALSE , eval=FALSE}
D <- rexp(100 , rate = 10) #generate exponentially distributed data
N <- length(D)
fr <- function(m) {   ## -LL for exponential
  m1 <- m[1]
  - N*(log(m)) + m*sum(D)
}
res <- optim(c(10) , fr , method = "L-BFGS-B" , lower = 0 ) #minimize -LL with boundary = 0
res$par
```

```{r , fig.width = 4 , fig.height = 3 , echo=FALSE , eval=FALSE}
df <- read.csv("MG_MZ_12uMdata.csv" , header = FALSE)
d <- as.matrix(df)
data <- matrix( d , ncol = 1)
data <- data[data !=0]
qplot(data , binwidth = 25 , xlab = "lifetime")
N <- length(data)
fr <- function(m) {   ## -LL for exponential
  m1 <- m[1]
  - N*(log(m)) + m*sum(data)
}
res <- optim(c(100) , fr , method = "L-BFGS-B" , lower = 0.0001 ) #minimize -LL with boundary = 0
print(res$par)# print rate
```


<h4>Bayesian parameter estimation for a Gamma distribution</h4>

Now do the same with a gamma distribution, for which the probability distribution is $P(x|\alpha,\beta) = \frac{\beta^{\alpha}}{\Gamma(\alpha)}x^{\alpha - 1}e^{-\beta x}$.

Then the log likelihood function is $$LL = \sum_i[(\alpha -1)\text{log}(x_i) - \beta x_i] + n\alpha\text{log}(\beta) - n\text{log}(\Gamma(\alpha)).$$

For a uniform prior, the log posterior is the log likelihood plus a constant. We will fit a gamma (remember: multistep) distribution to the microtubule lifetime data in a minute. 

**Solution**:
```{r , fig.width = 8 , fig.height = 3 , message = FALSE , echo=TRUE}
data <- read.csv("MG_catastrophe_data/lifetimes_tubulin.csv" , check.names=FALSE) #load data from #.csv
x <- data$`12_uM`
Lp_gamma <- function( m ){
  shape = m[1]
  rate = m[2]
  R <- dgamma( x , shape , rate , log = TRUE)
  -sum(R) - log( rate ) - log(shape)
}
#fit_exp <- mle( Lp_exp , start = list( rate = 1) )
#summary(fit_exp)
res_gam <- optim(c(10.5 , 20) , Lp_gamma, method = "L-BFGS-B" , lower = c(1e-100,1e-100) ) 
#minimize -LL with boundary = 0
res_gam$par
```






```{r , fig.width = 4 , fig.height = 3 , echo=FALSE , eval=FALSE}
df <- read.csv("MG_MZ_12uMdata.csv" , header = FALSE)
d <- as.matrix(df)
data <- matrix( d , ncol = 1)
data <- data[data !=0]
qplot(data , binwidth = 25 , xlab = "lifetime")
N <- length(data )
fr <- function(m) {   ## -LL for exponential
  m1 <- m[1]
  m2 <- m[2]
   - N*m[1]*log(m[2]) + N*log(gamma(m[1])) - (m[1] - 1)*sum(log(data)) + m[2]*sum(data)
}
res <- optim(c(10,10) , fr , method = "L-BFGS-B" , lower = c(0.001,0.001) ) #minimize -LL with boundary = 0
res$par
```

**Exercise for the avid Workshop participant**: 
Plot the data with both models -- do so in two ways:

1. Histogram;
2. eCDF + model CDF!

Now we have two models: one looks like a better fit; the other has less parameters. Which is a better model? We'll get to that soon. First, as always, we need to talk about confidence intervals (or SDs) on our parameter estimates.


##Finding the standard deviation of the posterior

###The one-dimensional case

In the one-dimensional case, the *posterior mode* $\lambda_0$ satisfies the condition

$$\frac{dP}{d\lambda}|_{\lambda_0} = 0.$$

Technically,we also require that the 2nd derivative is $<0$. Once again, instead of working with $P$, we look to its logarithm $L$, which we Taylor expand about the point $\lambda = \lambda_0$:

$$L = L(\lambda_0) + \frac{1}{2}\frac{dL^2}{d\lambda^2}|_{\lambda_0}(\lambda - \lambda_0)^2 + \ldots ,$$

in which the linear term is missing as we $lambda_0$ is a local maximum. Ignoring higher-order terms yields

$$P \approx A\text{exp}[\frac{1}{2}\frac{dL^2}{d\lambda^2}|_{\lambda_0}(\lambda - \lambda_0)^2],$$

for a normalization constant $A$. This is none other than a Gaussian distribution with standard deviation

$$\sigma_{\lambda_0} = (-\frac{dL^2}{d\lambda^2}|_{\lambda_0})^{-1/2}.$$

Thus, to calculate the error bars $\sigma_{\lambda_0}$, we look at the 2nd derivative in the log-posterior space.

```{r , fig.width = 4 , fig.height = 3 , echo=TRUE , eval=TRUE}
library(numDeriv)
h <- hessian( Lp_exp , res_exp$par ) #calculate 2nd derivative
std_dev_exp <- sqrt(1/h) #this is the standard deviation
#print posterior mode + sd
cat('mu = ' , signif(res_exp$par,2) , '+-' , signif(std_dev_exp[1,1] , 4) , '' )
```
###The two-dimensional case

Similarly, in two dimensions, the standard deviation of individual parameter estimates is related to the [Hessian matrix of 2nd derivatives](https://en.wikipedia.org/wiki/Hessian_matrix) $\mathbf{H}$ (also see Sivia & Skilling, 2006, Chapter 3 for a great explanation of all of this).

Then the standard deviation $\sigma_i$ of the $i$th parameter is the $(i,i)-$th entry of $[-\mathbf{H}^{-1}]^{1/2}$.

```{r , fig.width = 4 , fig.height = 3 , echo=TRUE , eval=TRUE}
h <- hessian( Lp_gamma , res_gam$par )#calculate 2nd derivatives (Hessian)
cvm <- solve(h) #invert the matrix
std_dev <- sqrt(cvm) #square root to give sd
#print posterior mode + sd of alpha:
cat('alpha = ' , res_gam$par[1] , '+-'  , std_dev[1,1])
#print posterior mode + sd of beta:
cat('beta = ' , res_gam$par[2] , '+-'  , std_dev[2,2])
```


Now that we have two models (exponential , gamma) and parameters estimated in a Bayesian setting, how do we decide which is better??

#Bayesian Model Selection

##The probability of a model, given data


Lets say that we have a model $M$ and some data $D$. We want to know how to compute $P(M|D)$, the probability of the model $M$, in light of the data $D$. Bayes Theorem can help us with this:

$$P(M|D) = P(D|M)P(M).$$

**Question**: What is the *prior*, the *likelihood* and the *posterior* in this formulation?

Let's look at the term $P(D|M)$ in the case where the model $M$ has one free parameter $\lambda$ and let's first assume that $\lambda$ is discrete, that is, that it can take on only a finite number of values $\lambda_i$, indexed by $i$. Then

$$P(D|M) = \sum_i P(D , \lambda_i |M) = \sum_i P(D |M ,  \lambda_i)P(\lambda_i|M).$$

We are really interested in the case in which $\lambda$ is continuous & NOT discrete, for example, the binomial probability, which can take on infinitely many values between $0$ and $1$. The continuous analog of the equation above is

$$P(D|M) = \int P(D |M ,  \lambda)P(\lambda|M)d\lambda.$$


Lets simplify the scenario slightly by assuming a *uniform prior* on $\lambda$. Then $P(\lambda|M) = \frac{1}{\lambda_{max}-\lambda_{min}},$ for $\lambda_{min} < \lambda < \lambda_{max}$.

If we also once again make the Gaussian assumption around the *posterior mode* $\lambda_0$(which does hold much of the time), we see that

$$P(D|M ,  \lambda) = P(D|M ,  \lambda_0)\text{exp}[-\frac{(\lambda-\lambda_0)^2}{2\delta\lambda^2}].$$

It follows that

$$P(D|M) = \frac{P(D|\lambda_0, M)\delta\lambda\sqrt{2\pi}}{\lambda_{max}-\lambda_{min}}.$$

Then, given two models $M_1, M_2$, with one free parameter each, $\lambda$ and $\mu$ respectively,

$$\frac{P(D|M_1)}{P(D|M_2)} = \frac{P(M_1)}{P(M_2)}\frac{P(D|\lambda_0 , M_1)}{P(D|\mu_0 , M_2)}\frac{\delta\mu(\lambda_{max}-\lambda_{min})}{\delta\lambda(\mu_{max}-\mu_{min})}.$$

But what is $\frac{P(M_1)}{P(M_2)}$?? If we initially have no reason to prefer one model over another, we assume that this ratio $=1$ and then 

$$\frac{P(D|M_1)}{P(D|M_2)} = \frac{P(D|\lambda_0 , M_1)}{P(D|\mu_0 , M_2)}\frac{\delta\mu(\lambda_{max}-\lambda_{min})}{\delta\lambda(\mu_{max}-\mu_{min})}.$$

##A two variable model

$$P(M|D) = P(D|\lambda_0,\mu_0)(\lambda_{max}-\lambda_{min}))(\mu_{max}-\mu_{min})[-\mathbf{H}^{-1}]^{1/2}.$$

##Example: models of microtubule lifetime

Then look at specifics of exponential vs gamma microtubule catastrophe.

#References

- Gardner MK, Zanic M, Gell C, Bormuth V, et al. 2011. Depolymerizing kinesins Kip3 and MCAK shape cellular microtubule architecture by differential control of catastrophe. Cell 147: 1092– 103.
- Data Analysis: A Bayesian Tutorial by Sivia & Skilling (2006) Oxford University Press
