---
title: "Notes on practical statistics IV"
output:
  html_document:
    toc: true
    fig_caption: true
    number_sections: true
---
<h1>An Introduction to Bayesian Inference</h1>

One Mode of Motivation: we have seen, when estimating parameters, that we can retrieve mean estimates and confidence intervals. But can we get the distribution? One method to do so is Bayesian paramater estimation model fitting.

#Bayes' Theorem

$$P(A|B) = \frac{P(B|A)P(A)}{P(B)}$$

Easy to prove: so do it.

#Bayes' Theorem and Parameter Estimation

##Priors, likelihoods, posteriors and parameter estimation

**Example**: biased coin flip with binomial probability $P(H)=\lambda$ and data $D$ consisting of $k$ heads and $n-k$ tails.

**Challenge**: Given the data $D$, we want to estimate the parameter $\lambda$.

**Method**: Using Bayes' Theorem, we see that

$$P(\lambda | D) = \frac{P(D | \lambda)P(\lambda)}{P(D)} \propto P( D | \lambda)P(\lambda).$$

In this equation, we call $P(\lambda)$ the *prior* (distribution), $P(D|\lambda)$ the *likelihood* (we've seen this before in MLE) and $P(\lambda | D)$ the *posterior* (distribution). The intuition behind the nomenclature is as follows: the *prior* is the distribution containing our knowledge about $\lambda$ prior to the introduction $D$ & the *posterior* is the distribution containing our knowledge about $\lambda$ after considering the data $D$.

**Note**: What is the prior? Really, what do we know about $\lambda$ before we see any data? Well, as it is a probability, we know that $0 \leq \lambda \leq 1$. If we haven't flipped any coins yet, we don't know much else: so it seems logical that all values of $\lambda$ are equally like, i.e., $P(\lambda) = 1,$ for $0 \leq \lambda \leq 1$. This is known as an *uninformative prior* because it contains little information (there are other uninformative priors we may use in this situation, such as the *Jeffreys prior*, to be discussed later). People who like to hate on Bayesian inference tend to claim that the need to choose a prior makes Bayesian methods somewhat arbitrary, but as we'll now see, if you have enough data, the likelihood dominates over the prior and the latter doesn't matter so much.

**Example**: Binomial -- provide derivation and figures below are for $n=100,k=40$ & $n=1000,k=450$, respectively.

```{r , fig.width = 8 , fig.height = 3 , message = FALSE , echo=FALSE}
#see here: http://www.r-bloggers.com/r-and-bayesian-statistics/
library( ggplot2 )
require(gridExtra)
n1 <- 100
k1 <- 40
lambda <- seq( 0 , 1 , by = 0.01)
prior <- rep( 1 , length(lambda))
likelihood1 <- lambda**k1*(1-lambda)**(n1-k1)
posterior1 <- likelihood1*prior
plot1 <- qplot( lambda , posterior1, geom = "line" ) 
##
n2 <- 1000
k2 <- 450
likelihood2 <- lambda**k2*(1-lambda)**(n2-k2)
posterior2 <- likelihood2*prior
plot2 <- qplot( lambda , posterior2, geom = "line" ) 
grid.arrange(plot1, plot2, ncol=2)
```

**Exercise 1**:

Compute and plot the posterior for two priors (uniform & 'crazy coin') for a variety of $n = 1 , 4 , 16 , 64, 256 , 1024 , 4096$.

**What to report once you have the posterior distribution**:

- The **posterior mode** (note that, in the case of a uniform prior, this is precisely the **maximum likelihood estimate**);
- The **standard deviation of the posterior** if it is approximately Gaussian around the mode;
- **$95\%$ confidence intervals** otherwise;
- Report **the distribution**!

**Slight tangent**: Now we see that the MLE we looked at in detail in Workshop III is a special case of Bayesian inference, we can view a number of our established techniques as a nested sequence:

$$\text{linear regression} \subset \text{nonlinear least squares} \subset \text{maximum likelihood estimation} \subset \text{Bayesian inference}$$

##Alternative priors

There is a subtle issue in choosing the uniform prior on the *binomial probability* $\lambda$. The issue is as follows: let's say that 

1. I choose the *uniform prior* on $\lambda$, claiming that I do so because I have *absolutely no prior knowledge* as to what $\lambda$ is;
2. Olivier Trottier (teaching assistant) is looking at the same data as me BUT Oli is thinking about the scientific question in terms of the *odds parameter* $\tau = \frac{\lambda}{1 - \lambda}$; Oli rightly feels that he has no *prior knowledge* as to what this $\tau$ is and thus chooses the uniform prior on $\tau$.

With a bit of algebra (transformation of variables), we can show that choosing the *uniform prior* on $\lambda$ amounts to choosing a decidedly *non-uniform* prior on $\tau$. So Oli and I have actually chosen different priors, using the same philosophy. How do we avoid this happening? Enter the **Jeffreys prior**.

<h4>The Jeffreys prior</h4>

The *Jeffreys prior* is invariant, in following sense: if I choose the Jeffreys prior on a set of variables and Oli chooses the Jeffreys prior on a transformation (reparametrization) of those variables, then we have both chosen the same prior.

We include the definition of the Jeffreys prior in the one-parameter case, for completeness. You will need to know a bit more about probability theory to understand the definition, however. If this is not the case, skip it or learn it!

**Definition**:

For the one-parameter case, the **Jeffreys prior** is defined to be

$$p(\theta) = \sqrt{I(\theta)},$$

where $I$ is the Fisher Information. See [here](https://eventuallyalmosteverywhere.wordpress.com/2013/05/10/bayesian-inference-and-the-jeffreys-prior/) & the Gelman book for further details.

**Example**:

In the case of the binomial distribution, the Jeffreys prior for $\lambda$ is

$$p(\lambda) = \frac{1}{\sqrt{\lambda(1-\lambda)}}.$$

#Computing the posterior mode & the standard deviation

##Finding the best estimate: the posterior mode

Recall that we need to find the *posterior mode*. Just as we saw in MLE, the numbers in question get super-small super-quickly so we actually minimize the *negative log-posterior*.

**Example** (uniform prior):
```{r , fig.width = 4 , fig.height = 3}
bin_data <- rbinom(1024, 1  , 0.75)
Lp <- function( p ){
  R <- dbinom( bin_data , 1, p ) #binomial function w/ probability p
  -sum(log(R)) #-ve log likelihood
}
res <- optim(c(0.5) , Lp , method = "L-BFGS-B" , lower = 0.0001 , upper = 0.999 ) #minimize -LL with boundary = 0
res$par
```

**Example** (Jeffreys prior):
```{r , fig.width = 4 , fig.height = 3}
bin_data <- rbinom(1024, 1  , 0.25)
Lp <- function( p ){
  R <- dbinom( bin_data , 1, p ) #binomial function w/ probability p
  -sum(log(R)) -log(1/sqrt(p*(1-p)))#-ve log likelihood
}
res <- optim(c(0.5) , Lp , method = "L-BFGS-B" , lower = 0.0001 , upper = 0.999 ) #minimize -LL with boundary = 0
res$par
```

**What about error bars in the Bayesian setting?** We'll check out how to compute them after a couple more examples.

###Bayesian parameter estimation for an exponential distribution

We're now going to check back in with our microtubule lifetime data from (Gardner et al., 2011) to perform parameter estimation for i) an exponential distribution & ii) a gamma distribution.

**Exercise**: Exponential distribution $P(x) = \frac{\text{exp}(-x/\mu)}{\mu}$. Assume uniform prior on $\mu$ (we will do the same using Jeffreys' prior), the posterior

$$P(\mu|D) \propto \prod_i \frac{\text{exp}(-x_i/\mu)}{\mu}$$

and the log-posterior $\text{log}P(\mu|D)= C + n\text{log}(1/\mu) - \frac{\sum x_i}{\mu}.$ So we wish to minimize the *negative log posterior*. We do so with the following code:

```{r , fig.width = 8 , fig.height = 3}
D <- rexp(100 , rate = 10) #generate exponentially distributed data
N <- length(D)
fr <- function(m) {   ## -LL for exponential
  m1 <- m[1]
  - N*(log(m)) + m*sum(D)
}
res <- optim(c(10) , fr , method = "L-BFGS-B" , lower = 0 ) #minimize -LL with boundary = 0
res$par
```
Note in this case that we can also do it analytically (Appendix). Now let's do the same with microtubule lifetime data:

```{r , fig.width = 4 , fig.height = 3}
df <- read.csv("MG_MZ_12uMdata.csv" , header = FALSE)
d <- as.matrix(df)
data <- matrix( d , ncol = 1)
data <- data[data !=0]
qplot(data , binwidth = 25 , xlab = "lifetime")
N <- length(data)
fr <- function(m) {   ## -LL for exponential
  m1 <- m[1]
  - N*(log(m)) + m*sum(data)
}
res <- optim(c(100) , fr , method = "L-BFGS-B" , lower = 0.0001 ) #minimize -LL with boundary = 0
print(1/res$par)# print rate
```
If it were exponential, the rate would be ~$0.0056/s$.

<h4>Bayesian parameter estimation for a Gamma distribution</h4>

Now do the same with a gamma distribution, for which the probability distribution is $P(x|\alpha,\beta) = \frac{\beta^{\alpha}}{\Gamma(\alpha)}x^{\alpha - 1}e^{-\beta x}$.

Then the log likelihood function is $$LL = \sum_i[(\alpha -1)\text{log}(x_i) - \beta x_i] + n\alpha\text{log}(\beta) - n\text{log}(\Gamma(\alpha)).$$

For a uniform prior, the log posterior is the log likelihood plus a constant. We will fit a gamma (remember: multistep) distribution to the microtubule lifetime data in a minute. First, we will do it to a synthesized dataset to make sure that our algorithm is working correctly:

```{r , fig.width = 4 , fig.height = 3}
data <- rgamma(10000 , shape = 3 , rate = 1)
N <- length(data )
qplot(data , binwidth =0.5)
fr <- function(m) {   ## -LL for exponential
  m1 <- m[1]
  m2 <- m[2]
   - N*m[1]*log(m[2]) + N*log(gamma(m[1])) - (m[1] - 1)*sum(log(data)) + m[2]*sum(data)
}
res <- optim(c(10,10) , fr , method = "L-BFGS-B" , lower = c(0.001,0.001) ) #minimize -LL with boundary = 0
res$par
```
It works :-).

Now for the microtubule lifetime data:

```{r , fig.width = 4 , fig.height = 3}
df <- read.csv("MG_MZ_12uMdata.csv" , header = FALSE)
d <- as.matrix(df)
data <- matrix( d , ncol = 1)
data <- data[data !=0]
qplot(data , binwidth = 25 , xlab = "lifetime")
N <- length(data )
fr <- function(m) {   ## -LL for exponential
  m1 <- m[1]
  m2 <- m[2]
   - N*m[1]*log(m[2]) + N*log(gamma(m[1])) - (m[1] - 1)*sum(log(data)) + m[2]*sum(data)
}
res <- optim(c(10,10) , fr , method = "L-BFGS-B" , lower = c(0.001,0.001) ) #minimize -LL with boundary = 0
res$par
```

Now plot the data with both models -- do so in two ways:

1. Histogram;
2. eCDF + model CDF!

Now we have two models: one looks like a better fit (gamma; n.b. way better fit); the other has less parameters. Which is a better model? We'll get to that soon. First, as always, we need to talk about confidence intervals (or SDs) on our parameter estimates.


##Finding the standard deviation of the posterior


And standard deviation? Look at the Hessian & the covariance matrices! More details coming.




Now we have 2 models: which is a better model? Segue into Bayesian model selection:

#Bayesian Model Selection

Introduce.

Then look at specifics of exponential vs gamma microtubule catastrophe.


More complex cases, for example, need more complex methods. Introduce an example and then discuss MCMC methods.

#Markov Chain Monte Carlo algorithm

**Description**:


**Example**:

```{r , fig.width = 6 , fig.height = 6 , message = FALSE , echo=FALSE}
#see here: http://www.r-bloggers.com/r-and-bayesian-statistics/
dat <- data.frame(year = c(2016,2012,2008,2004,2000,1996,1992,1988,1984,1980,1976,1972,1968,1964,1960,1956,1952,1948),
                  gdp.growth = c(NA,1.3,1.3,2.6,8,7.1,4.3,5.2,7.1,-7.9,3,9.8,7,4.7,-1.9,3.2,0.4,7.5),
                  net.approval = c(NA,-0.8,-37,-0.5,19.5,15.5,-18,10,20,-21.7,5,26,-5,60.3,37,53.5,-27,-6),
                  two.terms = c(1,0,1,0,1,0,1,1,0,0,1,0,1,0,1,0,1,1),
                  incumbent.vote = c(NA,52.0,46.3,51.2,50.3,54.7,46.5,53.9,59.2,44.7,48.9,61.8,49.6,61.3,49.9,57.8,44.5,52.4))
library(MCMCpack)
breg <- MCMCregress(incumbent.vote ~ gdp.growth + net.approval + two.terms, dat)
summary(breg); plot(breg)
```



```{r , fig.width = 6 , fig.height = 3 , message = FALSE , echo=FALSE}
#see here: http://www.r-tutor.com/bayesian-statistics/openbugs
library(R2OpenBUGS)
library(MASS)
y ~ dbin(p, N)
p ~ dbeta(1, 1)
model <- function() {
    p ~ dbeta(1, 1) # Prior
    y ~ dbin(p, N) # Likelihood 
}
tbl <- table(survey$Smoke) #smoker data
N <- as.numeric(sum(tbl)) #total number of students
y <- N - as.numeric(tbl["Never"]) # number that never smoked
data <- list("N", "y") #data variables
params <- c("p") #the variable(s) in our model
inits <- function() { list(p=0.5) } #we initialize p to be 0.5
#we wrap the initial values inside a list that is to be returned by a #function
#out <- bugs(data, inits, params, 
#+    model.file, n.iter=10000)
```

