---
title: "Notes on practical statistics III"
output:
  html_document:
    toc: true
    fig_caption: true
    number_sections: true
---
<h1>Maximum Likelihood Estimation, Linear Regression & Generalized Linear Models</h1>

#Maximum Likelihood Estimation:
Let's say that we want figure out the proportion of cases in which a mutation has a particular effect: for example, how often does a mutant die. If we think of survival as *heads* and death as *tails*, we can model this as a biased coin flip with $P(H) = p$ and $P(T) = 1 - p$.

Set-up: I flip a biased coin $n$ times and retrieve $k$ heads and $n-k$ tails, 

Questions:

1. What is my intuitive estimate of the probability of heads $P(H)=p$?
2. How certain can I be of this estimate? In other words, what type of confidence intervals can I place on it.
Answer: My estimate is $k/n$?

An intuitive answer to the 1st question is $\hat{p} = k/n$. We will now see why:

Recall that the binomial distribution is given by 

$$P(k \text{ heads }| n \text{ tosses }, p) = {n\choose k}p^k(1 - p)^{n-k}.$$

```{r , fig.width = 6 , fig.height = 3 , message = FALSE , echo=FALSE}
library( ggplot2 )
x <- seq(0,50)
y1 <- dbinom(x , size = 20 ,prob = 0.7 )
y2 <- dbinom(x , size = 20 ,prob = 0.5 )
y3 <- dbinom(x , size = 40 ,prob = 0.5 )
df1 <- as.data.frame( y1 ) #turn vector into dataframe
colnames(df1) = 'value' #change column names
df2 <- as.data.frame( y2 ) #turn vector into dataframe
colnames(df2) = 'value' #change column names
df3 <- as.data.frame( y3 ) #turn vector into dataframe
colnames(df3) = 'value' #change column names
df1$type <- 'size = 20, p = 0.7' #create a new column called 'type'
df2$type <- 'size = 20, p = 0.5' #create a new column called 'type'
df3$type <- 'size = 40, p = 0.5' #create a new column called 'type'
df <- rbind(df1,df2,df3) #build dataframe from these data
df$x <- x
p <- ggplot( df , aes( x = x , y = value, color = type))
p + geom_point() + scale_fill_discrete()
```

Now in our case we don't want the probability of flipping a certain number of heads, given $p$, we want the likelihood that $p(H)=p$, given a certain number of heads! To this end, we define the *likelihood* of any given $p$ given the data $D$ to be the probability of that data $D$, given $p(H) = p$:

$$\mathcal{L}(p | k \text{ heads}) := P(k \text{ heads }| n \text{ tosses }, p) = {n\choose k}p^k(1 - p)^{n-k}.$$

The *maximum likelihood estimate* (MLE) $\hat{p}$ of $p$ is the value that maximizes the likelihood function $\mathcal{L}(p | k \text{ heads})$. A little bit of calculus & algebra later (see Appendix) reveals that

$$\hat{p} = \frac{n}{k},$$

precisely as our intuition told us.

Now how confident can we be of this estimate? Our intuition is merely qualitative when it comes to this question: all we can really say is that the more data we have, the more we can be sure of our estimate. To quantitate this intuition, we use the Central Limit Theorem.

##The CLT and confidence intervals on a biased coin flip

First notice that $\hat{p}=k/n$ is also $S_n$, an estimate of the mean of the distribution given by $P(1)=p, P(0)=1-p$ (that is, we call a *head* 1 and a tail *0*). Then the Central Limit Theorem tells us that as $n$ gets large (in practice, $n>20$ should be fine), $\hat{p}=S_n$ is distributed like $N(\mu,\sigma^2/n),$ where $\mu$ and $\sigma^2$ are the mean and variance, respectively, of the binomial distribution. Hence $\hat{p}$ is ~normally distributed with mean $p$ and standard deviation $\sqrt{\frac{p(1-p)}{n}}$.

Note: the standard deviation of this estimate is a maximum when $p=0.5$, that is, the largest uncertainty occurs when the coin is fair!

Moreover, as $\hat{p}$ is normally distributed, $95\%$ confidence intervals on the estimate $\hat{p}$ are given by
$$\hat{p}\pm 1.96\times\text{SD}(\hat{p}).$$
When $p = 0.5$, this is
$\approx  \frac{k}{n} \pm \frac{1}{\sqrt{n}}$ and when $n=400$, for example, this is $\approx 0.5 \pm 0.05$.
If, on the other hand, $P(H)=0.75$, $1.96\times\text{SD}\approx 1/\sqrt{4n/3}$ and to get the same size $95\%$ confidence intervals, we only require 300 tosses of the coin.

```{r , fig.width = 9 , fig.height = 5 , message = FALSE , echo=FALSE}
p1 <- 0.5
p2 <- 0.75
x <- seq(1,401 , by = 10)
y1 <- 1.96*sqrt(p1*(1-p1))/sqrt(x)
y2 <- 1.96*sqrt(p2*(1-p2))/sqrt(x)
df1 <- as.data.frame(cbind( x ,y1  ))
colnames( df1 ) <- c( 'n' , 'SD')
df1$probability <- "p=0.5"
df2 <- as.data.frame(cbind( x ,y2  ))
colnames( df2 ) <- c( 'n' , 'SD')
df2$probability <- "p=0.75"
df <- rbind(df1 , df2)
pl <- ggplot( df , aes( x = n , y = SD , group = probability , colour = probability ))
ylabl <- expression(1.96%*%SD(hat(p)))
pl + geom_point(size = 3 , alpha = 1) + 
  scale_y_continuous(limits = c(0,0.2) , breaks=seq(0, 0.2, 0.05)) +
  ylab(ylabl) +
  theme(axis.title=element_text(size=22) ,
        axis.text=element_text(size=22))
```

- You can even simulate it yourself! Wait, you did at the end of the 1st class!

##A slight digression on *estimators* & *estimation theory*;

Suppose there is a fixed parameter $\theta$ that we cannot know directly, for example, the population mean. Then an *estimator* is a function that maps a sample drawn from the population to an estimate of $\theta$. An estimator of $\theta$ is commonly denoted by $\hat{\theta}$. You already know examples of estimators, such as

- Sample mean, which is an *estimator* of the population mean;
- Sample variance, which is an *estimator* of the population variance;

These two estimators are quite different however, in that the sample mean is an *unbiased estimator*, while the sample variance is a *biased estimator*. Let me explain: assume that we're sampling a distribution $P_\theta(x)=P(x|\theta)$ (where $\theta$ is some real number) and we have an estimator $\hat{\theta}$ of $\theta$. Then, recalling that $\text{E}_\theta[\hat{\theta}]$ is the *expected value* (or *mean*) of $\hat{\theta}$ over all possible samples drawn from $P_\theta(x)$, we define the *bias* of the estimator $\hat{\theta}$ to be
$$\text{Bias}_\theta[\hat{\theta}] = \text{E}_\theta[\hat{\theta}] - \theta.$$
If the bias of an estimator is zero, we call it *unbiased*. Otherwise, we call it *biased*.
Examples:

- The sample mean $\bar{x} =\sum x_i/n$ is an unbiased estimator;
- The (uncorrected) sample variance $=\sum(x_i - \bar{x})^2/n$ is a biased estimator: the usual definition of sample variance $=\sum(x_i - \bar{x})^2/(n-1)$ is, however, an unbiased estimator of sample variance.

#(simple) Linear Regression basics

The challenge: we have some data $D=\{x_i,y_i\}$ that looks like it could be linear, with some noise:


```{r , fig.width = 4 , fig.height = 3 , message = FALSE , echo=FALSE}
set.seed(42)
m <- 5
b <- 2
x <- seq( 0 , 1 , by = 0.01)
err <- rnorm( length(x) ) #normally distributed error
y <- m*x + b + err
df <- as.data.frame( cbind( x , y ))
p <- ggplot( df , aes( x , y ))
p + geom_point()
#p + geom_point() + stat_smooth( method = "lm" )
```

We form a (linear) model that $y=ax+b$ and we want to find the coefficients $a,b$ in the model that fit the data best, whatever that means. The method that most people know (if they know any) and the method that most computing software will implement, if not told to do otherwise, is *ordinary least squares*:

##Ordinary least squares (OLS)

For any $a,b$ we can compute $\varepsilon_i^2$, the square of distance between the y-coordinate of the data $y_i$ and the prediction of the model $ax_i+b$: this distance $\varepsilon_i$ is called the $i$th *residual* and $\varepsilon_i^2=(y_i-ax_i+b)^2$. The sum of the squares of the residuals is given by

$$SS_{red} = \sum\limits_i\varepsilon_i^2 = \sum\limits_i(y_i-ax_i+b)^2$$

and the OLS method for estimating $a,b$ is to find the parameters $\hat{a},\hat{b}$ that minimize $SS_{red}$. Notes: (i) in this case of simple (one independent variable) linear regression, there is an analytic solution for the estimates: $\hat{b} = r_{xy}\times\frac{\text{SD}_y}{\text{SD}_x}$ and $\hat{a} = \bar{y} - \hat{b}\bar{x}$, where $r_{xy}$ is the sample correlation coefficient between the $x_i$ and $y_i$; (ii) In more complex situations of *least-squares curve-fitting*, minimization is performed via an algorithm called *gradient descent*: if you're interested, check it out.

Key question: can we get error bars on these estimates? The answer is yes and wait to see how. R will readily plot the OLS estimate for you, along with $95\%$ confidence intervals:

```{r , fig.width = 4 , fig.height = 3 , message = FALSE}
p + geom_point() + stat_smooth( method = "lm" )
```

The more urgent questions we need to answer are 'Why and when does this OLS method work?' and 'What assumptions underly this method?' People (and software!) tend to apply this method blindly and to not realise that their approach may violate any number of its underlying assumptions, rendering it invalid. So when is it appropriate to use the ordinary least squares method and why?

###The assumptions underlying OLS

Make the assumption that each $y_i = ax_i + b + \varepsilon_i$ where $a$ and $b$ are real numbers and the $\varepsilon_i$ are  drawn independently from $N(0,\sigma^2)$, the Gaussian distribution with mean $0$ and variance $\sigma^2$:

$$P(\varepsilon_i) = \frac{1}{\sigma\sqrt{(2\pi)}}\text{exp}(-\frac{\varepsilon_i}{2\sigma^2}).$$

Then the OLS estimates $\hat{a},\hat{b}$ are none other than the maximum likelihood estimates for the parameters $a,b$ (see Appendix for a proof of this).

**ESSENTIAL:** In performing OLS, you have assumed:

- Your residuals are Gaussian with a mean of $0$;
- They are independent of one another (i.e., uncorrelated);
- Neither the mean nor the standard deviation is a function of $x$.

These points require slightly more explication: if the mean of the residuals in any thin vertical strip is non-zero, we call the residuals *biased* (otherwise, *unbiased*); if the variance of the residuals differs as a function of $x$, we call the residuals *heteroscedastic* (otherwise, *homoscedastic*):

<div style="horizontal-align:middle; text-align:center; width:400px; height=300px">
![Figure demsontrating residual bias and heteroscedasticity](residuals.png)
</div>

If assumptions are violated, not all is lost!! You just need to be a bit more cleverer in how you choose and or fit your model: it could be a problem with the signal (choose different model) OR it could be a problem with the noise (choose different noise, i.e., not OLS).


Coming up:

- Anscombe's Quartet;
- Application of of fitting a power-law to log-log data;
- Using bootstrapping to get error estimates on your parameters.

###Anscombe's Quartet demonstrates violations of OLS assumptions

Anscombe's Quarter consists of four datasets, each consisting of 11 data points. Each data set has the same $\bar{x}, \bar{y}, \text{SD}_x , \text{SD}_y$, correlation coefficient and linear regression line (OLS):

```{r , fig.width = 8 , fig.height = 6 , message = FALSE , echo = FALSE}
require(gridExtra)
attach( anscombe )
data <-  anscombe
###########
#in which we check it all out:
###########
#layout(matrix(c(1,2,3,4),2,2)) # optional 4 graphs/page 
###########
#do it 1
###########
m1 <- glm( y1~x1 , data = data , family = gaussian )

plot1 <- qplot( x1 , y1 , data = anscombe ) +
  stat_smooth( method = 'glm' , family = gaussian )
###########
#do it 2
###########
m2 <- glm( y2~x2 , data = data , family = gaussian )
plot2 <- qplot( x2 , y2 , data = anscombe ) +
  stat_smooth( method = 'glm' , family = gaussian )
###########
#do it 3
###########
m3 <- glm( y3~x3 , data = data , family = gaussian )

plot3 <- qplot( x3 , y3 , data = anscombe ) +
  stat_smooth( method = 'glm' , family = gaussian )
###########
#do it 4
###########
m4 <- glm( y4~x4 , data = data , family = gaussian )
plot4 <- qplot( x4 , y4 , data = anscombe ) +
  stat_smooth( method = 'glm' , family = gaussian )
grid.arrange(plot1, plot2, plot3, plot4 , ncol=2)
```

What would the residuals look like here?? Check 'em out:

```{r , fig.width = 8 , warning = FALSE, fig.height = 6 , message = FALSE , echo = FALSE}
res1 <-ggplot( m1 , aes( .fitted , .resid)) +
  geom_hline( yinctercept = 0 ) +
  geom_point() +
  geom_smooth( se = F)
###res1
res1 <-ggplot( m1 , aes( .fitted , .resid)) +
  geom_hline( yinctercept = 0 ) +
  geom_point() +
  geom_smooth( se = F)
###res2
res2 <-ggplot( m2 , aes( .fitted , .resid)) +
  geom_hline( yinctercept = 0 ) +
  geom_point() +
  geom_smooth( se = F)
###res3
res3 <-ggplot( m3 , aes( .fitted , .resid)) +
  geom_hline( yinctercept = 0 ) +
  geom_point() +
  geom_smooth( se = F)
###res4
res4 <-ggplot( m4 , aes( .fitted , .resid)) +
  geom_hline( yinctercept = 0 ) +
  geom_point() +
  geom_smooth( se = F)
grid.arrange(res1, res2, res3, res4 , ncol=2)
```

###Obtaining confidence intervals on your estimates using bootstrapping

The easiest way to retrieve confidence intervals on your parameter estimates is by *bootstrapping* (all this means is 'sampling with replacement', as we shall see).

The algorithm is as follows:

1. Perform the fit to get parameter estimates for the model $y=\hat{f}(x)$;
2. Boostrap the residuals (i.e., sample with replacement) and add the new sampled residuals to the model to get another dataset $D'$ -- that is, for every original data point $\{x_i , y_i \}$, randomly select a residual $\varepsilon_j$ and generate a new data point $\{x_i , y_i' \}$, where $y_i'=\hat{f}(x)+\varepsilon_j$ -- do this for every original data point to retrieve a new, synthesized (bootstrapped) data set $D' = \{x_i , y_i' \}_{i\in I}$;
3. Perform another fit on the new bootstrapped data to retrieve new paramater estimates $\hat{a'},\hat{b'}$.
4. Perform steps 2-3 above $n$ (commonly $=1000$) times. Then we have $n$ estimates for $a,b$, giving us a distribution, and we can look at the statistics of these distributions, such standard deviation, variance and confidence intervals.

This is all very straightforward to implement in R (for more, see http://www.statmethods.net/advstats/bootstrapping.html): 

```{r , fig.width = 4 , fig.height = 3 }
library( boot )
bs <- function(formula, data, indices) {
  d <- data[indices,] # allows boot to select sample 
  fit <- lm(formula, data=d)
  return(coef(fit)) 
} 
results <- boot(data=df, statistic=bs, 
                R=1000, formula=y~x)
print(results)
```

##Modeling a power law

Power laws occur all throughout biology (see Kleiber's Law & Geoff West papers); they are also the primary mode of thinking about scaling in biology, which is currently very sexy!

```{r , fig.width = 4 , fig.height = 3 , message = FALSE , echo=FALSE}
set.seed(100)
n <- 0.55
x <- seq( 0 , 200 , by = 2)
err <- rnorm( length(x) ) #normally distributed error
y <- x**n + err
df1 <- as.data.frame( cbind( x , y ))
p <- ggplot( df1 , aes( x , y ))
p + geom_point()
#p + geom_point() + stat_smooth( method = "lm" )
```

Discuss log-log + OLS.

Then move to nonlinear curve-fitting and follow this example: http://www.walkingrandomly.com/?p=5254

```{r , fig.width = 4 , fig.height = 3 }
# some starting values
n1 <- 10
# do the fit
fit = nls(y ~ x**n, start=list(n = n1) , control = list(maxiter = 500))
# summarise
summary(fit)
```

#Generalize to Least Squares Curve Fitting ($\chi^2$-test);

- Aaaaaand this is actually a special case of MLE!

#Then GLMs (to handle non-normal error).

- Shot noise. What else?
- Mention lasso & ridge regressions? Check out why this may be important for experimentalists.