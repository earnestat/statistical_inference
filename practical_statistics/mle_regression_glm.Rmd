---
title: "Notes on practical statistics III"
output:
  html_document:
    toc: true
    fig_caption: true
    number_sections: true
---
<h1>Maximum Likelihood Estimation, Linear Regression & Generalized Linear Models</h1>

#Maximum Likelihood Estimation:
Let's say that we want figure out the proportion of cases in which a mutation has a particular effect: for example, how often does a mutant die. If we think of survival as *heads* and death as *tails*, we can model this as a biased coin flip with $P(H) = p$ and $P(T) = 1 - p$.

Set-up: I flip a biased coin $n$ times and retrieve $k$ heads and $n-k$ tails, 

Questions:

1. What is my intuitive estimate of the probability of heads $P(H)=p$?
2. How certain can I be of this estimate? In other words, what type of confidence intervals can I place on it.
Answer: My estimate is $k/n$?

An intuitive answer to the 1st question is $\hat{p} = k/n$. We will now see why:

Recall that the binomial distribution is given by 

$$P(k \text{ heads }| n \text{ tosses }, p) = {n\choose k}p^k(1 - p)^{n-k}.$$

```{r , fig.width = 6 , fig.height = 3 , message = FALSE , echo=FALSE}
library( ggplot2 )
x <- seq(0,50)
y1 <- dbinom(x , size = 20 ,prob = 0.7 )
y2 <- dbinom(x , size = 20 ,prob = 0.5 )
y3 <- dbinom(x , size = 40 ,prob = 0.5 )
df1 <- as.data.frame( y1 ) #turn vector into dataframe
colnames(df1) = 'value' #change column names
df2 <- as.data.frame( y2 ) #turn vector into dataframe
colnames(df2) = 'value' #change column names
df3 <- as.data.frame( y3 ) #turn vector into dataframe
colnames(df3) = 'value' #change column names
df1$type <- 'size = 20, p = 0.7' #create a new column called 'type'
df2$type <- 'size = 20, p = 0.5' #create a new column called 'type'
df3$type <- 'size = 40, p = 0.5' #create a new column called 'type'
df <- rbind(df1,df2,df3) #build dataframe from these data
df$x <- x
p <- ggplot( df , aes( x = x , y = value, color = type))
p + geom_point() + scale_fill_discrete()
```

Now in our case we don't want the probability of flipping a certain number of heads, given $p$, we want the likelihood that $p(H)=p$, given a certain number of heads! To this end, we define the *likelihood* of any given $p$ given the data $D$ to be the probability of that data $D$, given $p(H) = p$:

$$\mathcal{L}(p | k \text{ heads}) := P(k \text{ heads }| n \text{ tosses }, p) = {n\choose k}p^k(1 - p)^{n-k}.$$

The *maximum likelihood estimate* (MLE) $\hat{p}$ of $p$ is the value that maximizes the likelihood function $\mathcal{L}(p | k \text{ heads})$. A little bit of calculus & algebra later (see Appendix) reveals that

$$\hat{p} = \frac{n}{k},$$

precisely as our intuition told us.

Now how confident can we be of this estimate? Our intuition is merely qualitative when it comes to this question: all we can really say is that the more data we have, the more we can be sure of our estimate. To quantitate this intuition, we use the Central Limit Theorem.

##The CLT and confidence intervals on a biased coin flip

First notice that $\hat{p}=k/n$ is also $S_n$, an estimate of the mean of the distribution given by $P(1)=p, P(0)=1-p$ (that is, we call a *head* 1 and a tail *0*). Then the Central Limit Theorem tells us that as $n$ gets large (in practice, $n>20$ should be fine), $\hat{p}=S_n$ is distributed like $N(\mu,\sigma^2/n),$ where $\mu$ and $\sigma^2$ are the mean and variance, respectively, of the binomial distribution. Hence $\hat{p}$ is ~normally distributed with mean $p$ and standard deviation $\sqrt{\frac{p(1-p)}{n}}$.

Note: the standard deviation of this estimate is a maximum when $p=0.5$, that is, the largest uncertainty occurs when the coin is fair!

Moreover, as $\hat{p}$ is normally distributed, $95%$ confidence intervals on the estimate $\hat{p}$ are given by
$$\hat{p}\pm 1.96\times\text{SD}(\hat{p}).$$
When $p = 0.5$, this is
$\approx  \frac{k}{n} \pm \frac{1}{\sqrt{n}}$ and when $n=400$, for example, this is $\approx 0.5 \pm 0.05$.
If, on the other hand, $P(H)=0.75$, $1.96\times\text{SD}\approx 1/\sqrt{4n/3}$ and to get the same size $95%$ confidence intervals, we only require 300 tosses of the coin.

```{r , fig.width = 9 , fig.height = 5 , message = FALSE , echo=FALSE}
p1 <- 0.5
p2 <- 0.75
x <- seq(1,401 , by = 10)
y1 <- 1.96*sqrt(p1*(1-p1))/sqrt(x)
y2 <- 1.96*sqrt(p2*(1-p2))/sqrt(x)
df1 <- as.data.frame(cbind( x ,y1  ))
colnames( df1 ) <- c( 'n' , 'SD')
df1$probability <- "p=0.5"
df2 <- as.data.frame(cbind( x ,y2  ))
colnames( df2 ) <- c( 'n' , 'SD')
df2$probability <- "p=0.75"
df <- rbind(df1 , df2)
pl <- ggplot( df , aes( x = n , y = SD , group = probability , colour = probability ))
ylabl <- expression(1.96%*%SD(hat(p)))
pl + geom_point(size = 3 , alpha = 1) + 
  scale_y_continuous(limits = c(0,0.2) , breaks=seq(0, 0.2, 0.05)) +
  ylab(ylabl) +
  theme(axis.title=element_text(size=22) ,
        axis.text=element_text(size=22))
```

- You can even simulate it yourself! Wait, you did at the end of the 1st class!

##A slight digression on *estimators* & *estimation theory*;

Suppose there is a fixed parameter $\theta$ that we cannot know directly, for example, the population mean. Then an *estimator* is a function that maps a sample drawn from the population to an estimate of $\theta$. An estimator of $\theta$ is commonly denoted by $\hat{\theta}$. You already know examples of estimators, such as

- Sample mean, which is an *estimator* of the population mean;
- Sample variance, which is an *estimator* of the population variance;

These two estimators are quite different however, in that the sample mean is an *unbiased estimator*, while the sample variance is a *biased estimator*. Let me explain: assume that we're sampling a distribution $P_\theta(x)=P(x|\theta)$ (where $\theta$ is some real number) and we have an estimator $\hat{\theta}$ of $\theta$. Then, recalling that $\text{E}_\theta[\hat{\theta}]$ is the *expected value* (or *mean*) of $\hat{\theta}$ over all possible samples drawn from $P_\theta(x)$, we define the *bias* of the estimator $\hat{\theta}$ to be
$$\text{Bias}_\theta[\hat{\theta}] = \text{E}_\theta[\hat{\theta}] - \theta.$$
If the bias of an estimator is zero, we call it *unbiased*. Otherwise, we call it *biased*.
Examples:

- The sample mean $\bar{x} =\sum x_i/n$ is an unbiased estimator;
- The (uncorrected) sample variance $=\sum(x_i - \bar{x})^2/n$ is a biased estimator: the usual definition of sample variance $=\sum(x_i - \bar{x})^2/(n-1)$ is, however, an unbiased estimator of sample variance.

#(simple) Linear Regression basics

The challenge: we have some data $D=\{x_i,y_i\}$ that looks like it could be linear, with some noise:


```{r , fig.width = 4 , fig.height = 3 , message = FALSE , echo=FALSE}
m <- 5
b <- 2
x <- seq( 0 , 1 , by = 0.01)
err <- rnorm( length(x) ) #normally distributed error
y <- m*x + b + err
df <- as.data.frame( cbind( x , y ))
p <- ggplot( df , aes( x , y ))
p + geom_point()
#p + geom_point() + stat_smooth( method = "lm" )
```

We form a (linear) model that $y=ax+b$ and we want to find the coefficients $a,b$ in the model that fit the data best, whatever that means. The method that most people know (if they know any) and the method that most computing software will implement, if not told to do otherwise, is *ordinary least squares*:

##Ordinary least squares (OLS)

For any $a,b$ we can compute $\varepsilon_i^2$, the square of distance between the y-coordinate of the data $y_i$ and the prediction of the model $ax_i+b$: this distance $\varepsilon_i$ is called the $i$th *residual* and $\varepsilon_i^2=(y_i-ax_i+b)^2$. The sum of the squares of the residuals is given by

$$SS_{red} = \sum\limits_i\varepsilon_i^2 = \sum\limits_i(y_i-ax_i+b)^2$$

and the OLS method for estimating $a,b$ is to find the parameters $\hat{a},\hat{b}$ that minimize $SS_{red}$. Notes: (i) in this case of simple (one independent variable) linear regression, there is an analytic solution for the estimates: $\hat{b} = r_{xy}\times\frac{\text{SD}_y}{\text{SD}_x}$ and $\hat{a} = \bar{y} - \hat{b}\bar{x}$, where $r_{xy}$ is the sample correlation coefficient between the $x_i$ and $y_i$; (ii) In more complex situations of *least-squares curve-fitting*, minimization is performed via an algorithm called *gradient descent*: if you're interested, check it out.


- method 
- residuals etc...
- including error bars on parameter estimates, boostrapping --yes!!).

```{r , fig.width = 4 , fig.height = 3 , message = FALSE , echo=FALSE}
m <- 5
b <- 2
x <- seq( 0 , 1 , by = 0.01)
err <- rnorm( length(x) ) #normally distributed error
y <- m*x + b + err
df <- as.data.frame( cbind( x , y ))
p <- ggplot( df , aes( x , y ))
#p + geom_point()
p + geom_point() + stat_smooth( method = "lm" )
```


#Generalize to Least Squares Curve Fitting ($\chi^2$-test);

- Aaaaaand this is actually a special case of MLE!

#Then GLMs (to handle non-normal error).

- Shot noise. What else?
- Mention lasso & ridge regressions? Check out why this may be important for experimentalists.