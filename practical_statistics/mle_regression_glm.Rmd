---
title: "Notes on practical statistics III"
output:
  html_document:
    toc: true
    fig_caption: true
    number_sections: true
---
<h1>Maximum Likelihood Estimation, Linear Regression & Generalized Linear Models</h1>

#Maximum Likelihood Estimation:
Let's say that we want figure out the proportion of cases in which a mutation has a particular effect: for example, how often does a mutant die. If we think of survival as *heads* and death as *tails*, we can model this as a biased coin flip with $P(H) = p$ and $P(T) = 1 - p$.

Set-up: I flip a biased coin $n$ times and retrieve $k$ heads and $n-k$ tails, 

Questions:

1. What is my intuitive estimate of the probability of heads $P(H)=p$?
2. How certain can I be of this estimate? In other words, what type of confidence intervals can I place on it.
Answer: My estimate is $k/n$?

An intuitive answer to the 1st question is $\hat{p} = k/n$. We will now see why:

Recall that the binomial distribution is given by 

$$P(k \text{ heads }| n \text{ tosses }, p) = {n\choose k}p^k(1 - p)^{n-k}.$$

```{r , fig.width = 6 , fig.height = 3 , message = FALSE , echo=FALSE}
library( ggplot2 )
x <- seq(0,50)
y1 <- dbinom(x , size = 20 ,prob = 0.7 )
y2 <- dbinom(x , size = 20 ,prob = 0.5 )
y3 <- dbinom(x , size = 40 ,prob = 0.5 )
df1 <- as.data.frame( y1 ) #turn vector into dataframe
colnames(df1) = 'value' #change column names
df2 <- as.data.frame( y2 ) #turn vector into dataframe
colnames(df2) = 'value' #change column names
df3 <- as.data.frame( y3 ) #turn vector into dataframe
colnames(df3) = 'value' #change column names
df1$type <- 'size = 20, p = 0.7' #create a new column called 'type'
df2$type <- 'size = 20, p = 0.5' #create a new column called 'type'
df3$type <- 'size = 40, p = 0.5' #create a new column called 'type'
df <- rbind(df1,df2,df3) #build dataframe from these data
df$x <- x
p <- ggplot( df , aes( x = x , y = value, color = type))
p + geom_point() + scale_fill_discrete()
```

Now in our case we don't want the probability of flipping a certain number of heads, given $p$, we want the likelihood that $p(H)=p$, given a certain number of heads! To this end, we define the *likelihood* of any given $p$ given the data $D$ to be the probability of that data $D$, given $p(H) = p$:

$$\mathcal{L}(p | k \text{ heads}) := P(k \text{ heads }| n \text{ tosses }, p) = {n\choose k}p^k(1 - p)^{n-k}.$$

The *maximum likelihood estimate* (MLE) $\hat{p}$ of $p$ is the value that maximizes the likelihood function $\mathcal{L}(p | k \text{ heads})$. A little bit of calculus & algebra later (see Appendix) reveals that

$$\hat{p} = \frac{n}{k},$$

precisely as our intuition told us.

Now how confident can we be of this estimate? Our intuition is merely qualitative when it comes to this question: all we can really say is that the more data we have, the more we can be sure of our estimate. To quantitate this intuition, we use the Central Limit Theorem.

##The CLT and confidence intervals on a biased coin flip

First notice that $\hat{p}=k/n$ is also $S_n$, an estimate of the mean of the distribution given by $P(1)=p, P(0)=1-p$ (that is, we call a *head* 1 and a tail *0*). Then the Central Limit Theorem tells us that as $n$ gets large (in practice, $n>20$ should be fine), $\hat{p}=S_n$ is distributed like $N(\mu,\sigma^2/n),$ where $\mu$ and $\sigma^2$ are the mean and variance, respectively, of the binomial distribution. Hence $\hat{p}$ is ~normally distributed with mean $p$ and standard deviation $\sqrt{\frac{p(1-p)}{n}}$.

Note: the standard deviation of this estimate is a maximum when $p=0.5$, that is, the largest uncertainty occurs when the coin is fair!

Moreover, as $\hat{p}$ is normally distributed, $95%$ confidence intervals on the estimate $\hat{p}$ are given by
$$\hat{p}\pm 1.96\times\text{SD}(\hat{p}).$$
When $p = 0.5$, this is
$\approx  \frac{k}{n} \pm \frac{1}{\sqrt{n}}$ and when $n=400$, for example, this is $\approx 0.5 \pm 0.05$.
If, on the other hand, $P(H)=0.75$, $1.96\times\text{SD}\approx 1/\sqrt{4n/3}$ and to get the same size $95%$ confidence intervals, we only require 300 tosses of the coin.

```{r , fig.width = 9 , fig.height = 5 , message = FALSE , echo=FALSE}
p1 <- 0.5
p2 <- 0.75
x <- seq(1,401 , by = 10)
y1 <- 1.96*sqrt(p1*(1-p1))/sqrt(x)
y2 <- 1.96*sqrt(p2*(1-p2))/sqrt(x)
df1 <- as.data.frame(cbind( x ,y1  ))
colnames( df1 ) <- c( 'n' , 'SD')
df1$probability <- "p=0.5"
df2 <- as.data.frame(cbind( x ,y2  ))
colnames( df2 ) <- c( 'n' , 'SD')
df2$probability <- "p=0.75"
df <- rbind(df1 , df2)
pl <- ggplot( df , aes( x = n , y = SD , group = probability , colour = probability ))
ylabl <- expression(1.96%*%SD(hat(p)))
pl + geom_point(size = 3 , alpha = 1) + 
  scale_y_continuous(limits = c(0,0.2) , breaks=seq(0, 0.2, 0.05)) +
  ylab(ylabl) +
  theme(axis.title=element_text(size=22) ,
        axis.text=element_text(size=22))
```


##A digression on *estimators* & *estimation theory*;

#Practical application of the CLT
to get the error on this estimate;

- You can even simulate it yourself!

#(simple) Linear Regression basics

- method 
- residuals etc...
- including error bars on parameter estimates, boostrapping --yes!!).

#Generalize to Least Squares Curve Fitting ($\chi^2$-test);

- Aaaaaand this is actually a special case of MLE!

#Then GLMs (to handle non-normal error).

- Shot noise. What else?
- Mention lasso & ridge regressions? Check out why this may be important for experimentalists.