---
title: "Notes on practical statistics I"
output:
  pdf_document:
    toc: true
    fig_caption: true
    number_sections: true
---
<h1>Statistical Motivations & Definitions</h1>
#There are many in which to plot data

A section on how to plot data (histograms, box plots, beeswarms), outlier detection etc... 

##First way to plot (univariate) data: histogram

When we say *univariate data*, all we mean is that there is one variable. The 1st way to plot such data is in a histogram. But first we need some data. There are 3 ways to get data into R:

1. Import data from a file external to R;
2. Load pre-existing data from within R;
3. Generate (synthesize) your own data using R's inbuilt random number generator.

We will go through each of these modes:

<h4>Importing data into R</h4>



The authors of (Gardner et al., 2011) have generously allowed us to use their dataset for this Workshop. We have it as a .csv ("Comma Separated Values") file. The data in this file is the lifetime (defined as the time from nucleation to catastrophe) of microtubules grown *in vitro* under different tubulin concentrations and each column is a different [Tb].

```{r qplot , fig.width = 4 , fig.height = 3 , message = FALSE}
#code chunk here:
library( ggplot2 ) #load ggplot2 library for handsome plotting structures
data <- read.csv("MG_catastrophe_data/lifetimes_tubulin.csv" , check.names=FALSE) #load data from .csv
##this is a dataframe -- a very important structure in R
colnames(data) #print the column names in the datframe 'data'
qplot( data$`12_uM` , xlab = "MT lifetime at 12uM") #plot a histogram of the data at 12uM
```

**Important Note**: The help() function in R will be one of your best friends; if, for example, you don't know what 'qplot' does, execute the following in your console:
```{r , fig.width = 4 , fig.height = 3 , message = FALSE}
help(qplot)
```


**Exercise**

1. Run the above code & figure out what each line does; notice that it throws a warnig to the console: what does this warning mean? Play around with binwidth parameter;
2. Load the microtubule lifetime data for the assay in which microtubules were grown in the presence of the microtubule associated protein (MAP) MCAK; plot lifetime histograms at various concentrations of MCAK;
3. How do these histrograms look (qualitatively) different from those of the microtubules grown in tubulin alone?

**Code for solution**

```{r , fig.width = 4 , fig.height = 3 , message = FALSE}
#code chunk here:
library( ggplot2 ) #load ggplot2 library for handsome plotting structures
dataM <- read.csv("MG_catastrophe_data/lifetimes_mcak.csv" , check.names=FALSE) #load data from .csv
colnames(dataM)
qplot( dataM$`9_nM_MCAK` , binwidth = 100 ,xlab = "MT lifetime at 9 nm MCAK") #plot a histogram of the data at 12uM
```

- Play around with bin size: a general rule-of thumb is # of bins $=\sqrt{n}$. If the bins are too small, you'll see a lot of sampling error noise; if the bins are too large you'll lose precision: for example, with large bins, gamma-distributed data can look exponentially distributed;
- There are many other potential rules of thumb, such Sturges' formula, the Rice rule, Doane's formula, Scott's normal reference rule, Freedman-Diaconis' choice and so on (see Scott, David W. (1992). Multivariate Density Estimation: Theory, Practice, and Visualization. New York: John Wiley & https://en.wikipedia.org/wiki/Histogram#Number_of_bins_and_width)
- One can also define bin width instead of the number of bins.

<h4>Loading data from within R</h4>

```{r , fig.width = 4 , fig.height = 3 , message = FALSE}
cars <- mtcars #pre-existing dataset in R
p1 <- ggplot(data = cars , aes(x=mpg)) #initialize plot to look at miles per gallon
p1 + geom_histogram( binwidth = 1 ) #plot histogram & play around with binwidth if desired
```

I know that this section is supposed to focus on plotting histograms of univariate data BUT ggplot2 makes plotting bivariate data so easy that coldn't help including a little bit here:
```{r , fig.width = 4 , fig.height = 3 , message = FALSE}
p2 <- ggplot( data = cars , aes( x = mpg , y = wt)) #initialize plot to look at the relationship
#between variables (bivariate) miles /gallon & weight (lb/1000)
p2 + geom_point() #plot a scatterplot
#plot scatter of mpg vs weight; plot linear fit + 95% confidence bands:
p2 + geom_point() + stat_smooth( method = "lm") + xlab("miles per gallon") + ylab("weight (lb/1000)") 
```


<h4>Generating your own data</h4>

R contains inbuilt random number generators that we can use to draw random numbers from, for example, uniform and Gaussian distributions. There are two points to note:

1. The default random number generator in R is the Mersenne Twister, which is actually a *pseudorandom number generator* (PRNG), not a *true random number generator*;
2. It is *very important* to set the seed of your PRNG before using it! This tells your computer where to start generating the random numbers; it follows that, if you run your code, you get the same random numbers and this is **essential for reproducible results**.

```{r , fig.width = 4 , fig.height = 3 , message = FALSE}
set.seed(42) #
x <- rnorm(1000 , mean = 0 , sd = 1) ##generate 1000 data points from a normal
#distribution with mean = 1, sd = 1
qplot( x ) #plot a histogram of the data
```

**Exercise**

1. Generate & plot data drawn from a (i) uniform distribution (for sample sizes of $n=100,500$) & (ii) an exponential distribution with rates $=1,3$.
2. For a dataset of your choice, plot the empirical probability density function instead of the histogram: all this means that the y-axis is normalized so that the $\sum\limits_\text{bins}f(\text{bins}) = 1$ & the interpretation is that $f$ provides the probability of a data point being in that bin.

*Note*: I haven't told you the functions used to generate these datasets. You'll need to find them somewhow! A huge part of coding is finding the right function. *Hint*: search engines are your friends.


##Second way to plot (univariate) data: (empirical) cumulative distribution function (ECDF or CDF)

The ECDF $F(x) = \frac{\text{number of data points} \leq x}{n}$, where $n$ is the total number of data points.

- The intuition is that this tells you the probability of being less than or to equal to the value of interest;
- This is a great way to visualize and compare distributions as there is no binning and thus no artefacts introduced by binning!
- A word of warning: there is correlation in the curve (as opposed to a histogram): All that I'm saying here is that if there is a large fluctuation for a low value, then this flucutation may be seen throughout the rest of the curve.

Let's first have a look at the ECDF for some of the microtubule lifetime dataset:

```{r , fig.width = 4 , fig.height = 3 , message = FALSE}
ggplot(data, aes(x = `12_uM`)) + stat_ecdf() + xlab("Microtubule lifetime") +
  ylab("ECDF")#plot ECDF of the data
```

**Exercise**

1. Generate $n=1000$ samples from a Gaussian distribution with mean$=0$, SD$=2$ and plot the ecdf (*hint*: you'll need to know how to turn a vector into a dataframe);

**Code for solution**

```{r , fig.width = 4 , fig.height = 3 , message = FALSE}
x <- rnorm(1000 , mean = 1 , sd = 2) ##generate 1000 data points from a normal
#distribution with mean = 1, sd = 1
df <- data.frame(x = x)
ggplot(df, aes(x)) + stat_ecdf() + ylab("ECDF") #plot ECDF of the data
```

##Third way to plot (univariate) data: box-plot

We now plot the microtubule lifetime data as a box-plot (to be defined below).

However, to do so, we want to have the microtubule lifetime data in a different form -- we would like a dataframe in which each row is a single experiment with 2 columns: (i) the recorded lifetime & (ii) the tubulin concentration of the experiment. 

```{r,  message = FALSE ,  echo = TRUE}
library( reshape )
mt_data <- melt( data ) #melt data as described above
colnames(mt_data) <- c("tubulin_concentration" , "lifetime") #rename the columns
mt_bp <- ggplot( mt_data , aes( x = factor(tubulin_concentration) , y = lifetime)) #initiate plot
mt_bp + geom_boxplot() + xlab("[Tb]") #plot the box-plot
```

**Questions**: 

1. Execution of the above code spat out a warning: what was this about?
2. When I initiated the plotting structure 'mt_bp', I turned 'tubulin_concentration' into a 'factor': what is a 'factor'?

##Third way to plot (univariate) data: column scatter plot (with jitter!)

For small amounts of data, you should plot all the data, not just the box-plot, which summarizes aspeects of the data. A column scatter plot (with jitter) is one way to achieve this and you can layer it on top of a box-plot:

```{r , fig.width = 6 , fig.height = 3 , message = FALSE , echo=FALSE}
#data from here: http://bl.ocks.org/mbostock/4061502#morley.csv
datamm <- read.csv("mm.csv")
p <- ggplot( datamm , aes( factor(Expt) , Speed ))
p + geom_boxplot() + geom_jitter(aes(colour = factor(Expt))) + guides(colour=guide_legend(title="Expt")) +
  xlab("Experiment") +ylab("Speed of Light (km/s minus 299,000)")
```

*Note*: the data here is from the famed Michelson-Morley experiment -- in which the data is the speed of light from different experiments.

**Question**: What is a problem with the way in which I have plotted both the box-plot and the jitter plot above?

##Third way to plot (univariate) data: beeswarm plot

If you have more data, it may overlap in a jitter plot: one way to avoid this is to use a beeswarm plot, in which the data is cleverly spread out:


```{r,  message = FALSE ,  echo = TRUE}
library(beeswarm)
data(breast)
beeswarm(time_survival ~ ER, data = breast,
         pch = 16, pwcol = 1 + as.numeric(event_survival),
         xlab = "", ylab = "Follow-up time (months)",
         labels = c("ER neg", "ER pos"))
legend("topright", legend = c("Yes", "No"),
       title = "Censored", pch = 16, col = 1:2)
```

In order to see more about the dataset used above, execute the following command:
```{r,  message = FALSE ,  echo = TRUE}
help(breast)
```

**Exercise**: create a beeswarm plot of the microtubule lifetime data with (i) tubulin concentration on the x-axis & (ii) lifetime on the y-axis.

**Question**: Is there the correct amount of data for a beeswarm plot here?

**Code for solution**
```{r,  message = FALSE ,  echo = TRUE}
beeswarm(lifetime ~ tubulin_concentration, data = mt_data,
         pch = 16, pwcol = 1 + 2*as.numeric(tubulin_concentration),
         xlab = "Tubulin concentration", ylab = "Lifetime")
```

#Summary Statistics

How can we numerically describe & summarise data? We use statistics: but what is a *statistic*? To liberally paraphrase Phil Gregory (in *Bayesian Logical Data Analysis for the Physical Sciences*), a <span style="color:red">*statistic*</span> is 'a function that maps any data set to a numerical measure'. (what Gregory actually wrote is that a *statistic* is 'any function of the observed random variables in a sample such that the function does not contain any unknown quantities'!). We use *summary statistics* to easily identify attributes such as *central tendency* and *varibility*, also called *dispersion*.

##Measures of Central Tendency

<h4>Mean</h4>
  
The *mean* of a dataset $D=\{x_i\}$ is the average $\mu = \frac{1}{n}\sum\limits_i x_i$. This is a very intuitive statistic & also one of the few statistics that most lay-people recognise as a quantity-of-interest: but why the hell is it of such import? Is it arbitrary? The short answer is 'no' and the long answer is too long. The easiest way to quantitatively motivate it here is that it *is the number that is, on average, closest to all of the data*, that is, it minimizes the the squared error sum (you can prove this with a bit of calculus & algebra!):

$$p(x) = \sum\limits_i(x - x_i)^2.$$


To calculate the mean of dataset, we use the function 'mean' in R:
```{r,  message = FALSE ,  echo = TRUE}
datamm_ep1 <- subset(datamm , Expt == 1) ##what have I done here?
mean_exp1 <- mean( datamm_ep1$Speed )
print( mean_exp1)
```

You can also calculate the mean of each column of a dataframe by using the 'apply' function:

```{r,  message = FALSE ,  echo = TRUE}
apply( data , 2 , mean)
```

*Question*: Whoa!! This reported NA's. Why did this happen? How can we avoid it?

*Code for solution*:
```{r,  message = FALSE ,  echo = TRUE}
#for more on 'apply' and its variants, see here: #https://nsaunders.wordpress.com/2010/08/20/a-brief-introduction-to-apply-in-r/
apply( data , 2 , function(x) mean(x,na.rm = TRUE))
```

<h4>Warning! The mean can be a crappy statistic</h4>

Note: the mean is not always a good statistic, particularly for skewed and bimodal distributions (more to come on this) or datasets with outliers!

Here are 3 cases in which the *mean* is not a great summary statistic to report:

1. a dataset with outliers (for example, if a reseacher accidentally adds a zero to a couple of data points when transcribing data; note: outliers will be defined below, when we have a few more tools under our statistical belt);
```{r , fig.width = 4 , fig.height = 3 ,message = FALSE}
x <- runif(10)
y <- 12
df <- as.data.frame( c( x , y) )
colnames(df) <- 'value'
df$type = "data"
m <- signif( mean(df$value) , 2)
llb <- paste("mean = " , m)
op <- ggplot( df , aes( x = type , y = value))
op + geom_point(alpha = 0.3) + scale_y_continuous(breaks=seq(0,13,2)) + xlab("")
op + geom_jitter( size = 4 , alpha = 0.5) +
  annotate("text",  label = llb ,  x = 1, y= 6) + scale_y_continuous(breaks=seq(0,13,2)) + xlab("")
```



2. data from a distribution with 2 peaks (e.g. heights of multiple genders or ethnicities )
```{r , fig.width = 8 , fig.height = 3 , message = FALSE}
##Galton height data: http://personality-project.org/r/html/heights.html
##Galton paper here: http://galton.org/essays/1880-1889/galton-1888-co-relations-royal-soc/galton_corr.html
source("multiplot.R")
library(psych)
data(heights)
m1 <- signif( mean(heights$height) , 3 )
llb1 <- paste("mean = " , m1)
p1 <- ggplot(data = heights , aes( x = height)) + geom_histogram(binwidth = 0.5) +
  annotate("text", x = 65, y = 60, label = llb1)
x <- rnorm( 1000 , mean = 0)
y <- rnorm( 1000 , mean = 5 )
d <- as.data.frame( c( x , y) )
colnames(d) <- 'value'
m2 <- signif( mean(d$value) , 3 )
llb2 <- paste("mean = " , m2)
p2 <- qplot(d$value) + xlab('value')+ annotate("text", x = 5, y = 160, label = llb2)
multiplot(p1, p2, cols=2)
```

- data from a skewed distribution (e.g. gamma distribtuion -- arises from multistep processes) -- in the example below, the mean is ~2 and the peak of the histogram occurs at ~1:
```{r , fig.width = 4 , fig.height = 3 ,message = FALSE}
x <- rgamma(10000 , 2 ,1)
qplot(x)
mean(x)
```

To deal with the problem of outliers, statisticians like to look at the *median* rather than the mean; if you're actually interested in the most likely data point, that is, where the highest peak is in the histogram, you look at the *mode*.

<h4>Median</h4>

To find the *median* of your dataset, list them from smallest to largest: the *median* is then defined to be the number that occurs in the middle of this list (or the average of the middle two, if there are an even number of data points; see *fully trimmed mid-range if interested*). Intuitively, what the *median* essentially does is split the data into a top 50% and bottom 50%. 

```{r , fig.width = 4 , fig.height = 3 ,message = FALSE}
mean(x)
median(x)
```

The median is very robust to outliers, in the sense that up to ~50% of the data can be contaminated to contain outlier and this will not alter the median. For example the dataset $\{1,2,2,3,4,5,5,6,7\}$ & $\{1,2,2,3,4,5,5,60,70\}$ both have a median of $4$ and yet their means differ significantly due to the presence of outliers in the latter. 

I include a demonstrative plot below. Although I have not introduced box-plots formally yet, the bar in the middle of the box is the median of the dataset: you can see that the medians are the same and yet the means are very different due to the presence of outliers:

```{r , fig.width = 4 , fig.height = 3 ,message = FALSE}
x <- c(1,2,2,3,4,5,5,6,7)
y <- c(1,2,2,3,4,5,5,60,70)
df <- data.frame(x,y)
print( c( mean(x) , mean(y)) )
dfm <- melt(df)
mp <- ggplot(dfm , aes( x = variable , y = value))
mp + geom_boxplot() + xlab("")
```

<h4>Mode</h4>

The *mode* is defined to be the value that is most likely to occur (i.e., where the peak in the histogram occurs). Give a visual example. When the histogram/probability distribution function has rwo or more modes, we call it *bi-modal* and *multi-modal* respectively. We will see that the mode can be of great interest later when performing parameter estimation (for example, miximum likelihood estimation and Bayesian parameter estimation). See an example of trimodality here:

```{r , fig.width = 8 , fig.height = 3 , message = FALSE , echo = FALSE}
source("multiplot.R")
x <- rnorm( 1000 , mean = 0)
y <- rnorm( 1000 , mean = 5 )
z <- rnorm( 750 , mean = 9)
d <- as.data.frame( c( x , y, z) )
colnames(d) <- 'value'
m2 <- signif( mean(d$value) , 3 )
llb2 <- paste("mean = " , m2)
p2 <- qplot(d$value) + xlab('value')+ annotate("text", x = 10, y = 190, label = llb2)
p2
```

Also see here for some interesting notes on relation between mode, median, mean & Pearson mode skewness: http://mathworld.wolfram.com/Mode.html

##Measures of Variability/Dispersion

<h4>Range</h4>
The range of a dataset $\{x_i\}$ is $r:=x_{max} - x_{min}$, the difference between the maximum and the minimum. Although it tells us the region in which all the data lies, it doesn't tell us anything else and it is also completely sensitive to outliers.

<h4>Variance & Standard Deviation</h4>

The *variance* is a measure of how far a dataset is spread out. Recalling the mean $\mu = \frac{1}{n}\sum\limits_i x_i$, the variance is defined to be

$$\sigma^2 = \frac{1}{n}\sum\limits_i(x_i-\mu)^2$$

and $\sigma$, the square root of the variance, is called the *standard deviation* of the data.

Cool note: having a two summary statistics such as a measure of central tendency (e.g., mean) and a measure of dispersion (e.g. variance), can be enough to fully describe many of the most important distributions. For example, an *exponential distribution* is characterizable by its mean alone, a *normal/Gaussian distribution* by its mean and variance, and a Pareto (power-law) distribution by its mean and variance (more on these below).

Key: variance and standard deviation are sensitive to outliers (see, e.g., http://www.statcan.gc.ca/edu/power-pouvoir/ch12/5214891-eng.htm). Also, standard deviation can give us insight into the existence of outliers:

```{r , fig.width = 4 , fig.height = 3 ,message = FALSE}
x <- c(1,2,2,3,4,5,5,6,7)
y <- c(1,2,2,3,4,5,5,60,70)
df <- data.frame(x,y)
print( apply( df , 2 , mean) )
print( apply( df , 2 , sd) )
```
**Question**: How can we see from mean & standard deviation of the above datasets that one of them may contain outliers?

**Exercise**: Compute the standard deviation of one of the microtubule lifetime datasets (for all concentrations) OR the the Michelson-Morley data (all experiments).

**Code for solution**:
```{r , fig.width = 4 , fig.height = 3 ,message = FALSE}
by(datamm$Speed, datamm$Expt , sd)
#OR
tapply(datamm$Speed, datamm$Expt , sd)
```

<h4>Interquartile Range</h4>

Recall that the *median* splits the data into a top 50% and a bottom 50%. We can similarly split these regions into their top and bottom halves: $Q_1$, the *1st quartile*, is defined to be the median of the bottom 50% (i.e. the median of the data consisting of all $x_i$ less than the median) & $Q_3$, the *3rd quartile*, is defined to be the median of the top 50% of the data. We then define the *interquatile range* to be $IQR = Q_3 - Q_1$, a measure of how spread out the data is (and it is a robust measure! particularly with respect to outliers).

Having defined the *median* and *quartiles*, we can now define a *box-plot* (also known as a box-and-whisker diagram), which provides a very useful way of visualizing data. In a box plot,

- The top and bottom of the box are $Q_3$ and $Q_1$, respectively;
- The band in the middle is the median;
- The ends of the top and bottom whiskers are $Q_3 + 1.5\times IQR$ & $Q_1 - 1.5\times IQR$, respectively (note: there are other possible conventions for the whiskers, however this is the rule that we shall use, unless specified otherwise);
- The outliers can be plotted as points; in fact, all of this quantile business allows us to give a precise definition of outliers as all data points that are above $Q_3 + 1.5\times IQR$ or that are below $Q_1 - 1.5\times IQR$ (there are other definitions, still an active area of research; see here, for example: https://www.siam.org/meetings/sdm10/tutorial3.pdf)

Here are boxplots from the famed Michelson-Morley experiment, in which the data is the speed of light in different experiments:
```{r , fig.width = 12 , fig.height = 3 , message = FALSE , echo=FALSE}
#data from here: http://bl.ocks.org/mbostock/4061502#morley.csv
data <- read.csv("mm.csv")
p <- ggplot( data , aes( factor(Expt) , Speed ))
p1 <- p + geom_boxplot() + xlab("Experiment") + ylab("Speed of Light (km/s minus 299,000)")
p2 <- p + geom_boxplot( notch=TRUE) + xlab("Experiment") + ylab("")
p3 <- p + geom_boxplot( notch=TRUE) + geom_jitter() + xlab("Experiment") + ylab("")
multiplot(p1, p2,p3, cols=3)
```
see about bagplots (bivariate boxplots) here: https://en.wikipedia.org/wiki/Bagplot



<h4>Standard Error of the Mean</h4>

Let's say we have drawn $n$ random, independent samples $X_1, X_2, \ldots , X_n$ from a probability distribution $P$, where $P$ has mean $\mu$ and variance $\sigma^2$ (define all of these things OR give an intuition via example; also note that you have only defined mean & variance for finite datasets as of yet, not for probability distributions). Noe the sample mean $\mu_X$ provides an estimate of the population mean $\mu$. The question is "how good is the estimate $\mu_X$ of $\mu$?" Another way to ask this is as follows: "if I performed the same experiment a number of times, that is, drew another $n$ samples and calculated the sample mean, what would be the standard deviation of all of these sample means?" The answer is that the *standard error of the sample mean* is

$$SE_{\bar{x}} = \frac{s}{\sqrt{n}},$$

where $n$ is the sample size and $s$ is the sample standard deviation. This follows, in fact, from the Central Limit Theorem, which we state below. First note, though, that there are diminishing returns of the number of data points with respest to the standard error, as it decreases as the inverse of $\sqrt(n)$, so to be 10 times as certain in your estimate of the mean, you actually require 100 times as many data points!

**Example of computation**:
```{r,  message = FALSE ,  echo = TRUE}
data <- read.csv("MG_catastrophe_data/lifetimes_tubulin.csv" , check.names=FALSE) #load data from .csv
mlt <- apply( data , 2 , function(x) mean(x , na.rm = TRUE)) #mean lifetime
n <- apply( data , 2 , function(x) sum(!is.na(x))) #number of data points
sem <- mlt/sqrt(n) #standard errors
print(sem)
```

#Central Limit Theorem

The Central Limit Theorem, in essence, tells about how the *sample mean*, as an *estimator* (to be defined), is itself distributed (it's also a random variable), in the limit of large samples:

Let $X_1, X_2, \ldots X_n$ be  a random sample drawn from a probability distribution $P$ (with mean $\mu$ and variance $\sigma^2$). The sample average is defined as

$$S_n = \frac{X_1 + X_2 + \ldots + X_n}{n}$$

and the Central Limit Theorem states that, as $n \to \infty$, $S_n \to N(\mu,\sigma^2/n)$. We will demonstrate part of its power when dealing with Student's t-test later. For the time being, let's look at what Francis Galton (Victorian statistician, progressive, polymath, sociologist, psychologist, anthropologist, eugenicist, tropical explorer, geographer, inventor, meteorologist, proto-geneticist and psychometrician)had to say about the CLT:

<blockquote><p style="font-size: 75%">I know of scarcely anything so apt to impress the imagination as the wonderful form of cosmic order expressed by the "Law of Frequency of Error". The law would have been personified by the Greeks and deified, if they had known of it. It reigns with serenity and in complete self-effacement, amidst the wildest confusion. The larger the mob, and the greater the apparent anarchy, the more perfect is its sway. It is the supreme law of Unreason. Whenever a large sample of chaotic elements are taken in hand and marshaled in the order of their magnitude, an unsuspected and most beautiful form of regularity proves to have been latent all along.</p></blockquote>

```{r, fig.width = 4 , fig.height = 3 ,  message = FALSE ,  echo = TRUE}
##Here we perform a simple demonstration of the Central Limit Theorem
##Random Variable defined by p(0)=p(1)=0.5. Draw n samples (e.g. n = 1000)
##& retrieve the mean. Do this m times (e.g. m = 10,000) and plot 
##the distribution of means:
n <- 1000 #number of samples for each trial
m <- 10000 #number of trials
x <- rbinom( m , n , 0.5 )/n # sample distribution and return vector of means
qplot(x , binwidth = 0.005) #plot histogram of means
```

```{r, fig.width = 4 , fig.height = 3 ,  message = FALSE ,  echo = TRUE}
##Here we perform a simple demonstration of the Central Limit Theorem
##We estimate pi (ratio of circle circumference to diameter) using a Monte Carlo method:
##the method is to drop n points inside a square of side length 2, then see how many points
##fall inside the circle of radius 1 inside the square. As area(square) = 4 & area(circle) = pi
##an estimate for pi is = 4*(number of points in circle)/(number of points in square).
##We then use this procedure to estimate pi m times and look at the statistics & distribution
##Of these estimates.
n <- 2000 #number of points to drop in the square
m <- 1000 #number of trials
#we write a small function here to approximate pi by the method described above
approx_pi <- function(n){
  points <- replicate(2, runif(n))  #generate points in the square
  distances <- apply( points , 1 , function(x) sqrt(x[1]^2 + x[2]^2)) #compute distance of each point from centre
  #see here for more on 'apply': 
  #https://nsaunders.wordpress.com/2010/08/20/a-brief-introduction-to-apply-in-r/
  points_in_circle <- sum(distances < 1) #count points in circle
  pi_approx <- 4*points_in_circle/n #approximate pi
  return(pi_approx) #function output designated here
}
x <- replicate(m,approx_pi(n)) #approximate pi m times
m <- mean(x) #compute mean of your estimates
qplot( x , binwidth = 0.01) #plot distribution of estimates
```

#An Incomplete Compendium of Distributions (perhaps this should occur earlier)

This section is a prototype: I need to flesh out equations, descriptions, motivaations and whatever else (eg. means, variances etc...). Oh, and include Figures: make it at least a little bit pleasing on the eye, please, Hugo!

##Normal (Gaussian) Distribution

Motivation:

- The Central Limit Theorem! WOWSER.
- Things in the phenomenal world are often normally distributed (this is why the term 'bell curve' is part of our general society's lexicon, rather than merely a technical term);
- Measurement error (e.g. using a ruler) is often normally distributed.

Equation: $P(x) = \frac{1}{\sigma\sqrt{2\pi}}\text{exp}(-\frac{(x - \mu)^2}{2\sigma^2})$

##Binomial Distribution

Motivation:

- Flipping coins!

Equation: $P(k|n) = {n\choose k}p^k(1 - p)^{n-k}$, where $p$ is the binomial probability (the probability of flipping heads).

##Poisson Distribution

Motivation:

- Counting things.

Equation: $P(k) = \frac{\lambda^k}{k!}e^{-\lambda}$, for $k \in \{0,1,2,3,\ldots\}$.

##Exponential Distribution

Motivation:

- Radioactive decay;
- Any Poisson Process has exponentially distributed waiting times (will prove below OR in appendix; will included numerical example in the exercises);

Equation: $P(x) = \frac{\text{exp}(-x/\mu)}{\mu}$.

##Gamma Distribution

Motivation:

- Results from multi-step processes (and thus generalizes the exponential distribution)

Equation: $P(x) = \frac{\beta^{\alpha}}{\Gamma(\alpha)}x^{\alpha - 1}e^{-\beta x}$.

##Other Distributions

This list is already long enough for the current purposes of introduction to distributions. There are also Pareto (power-law) distributions, Cauchy distributions, Laplace distributions and a very many more... (perhaps list more with a reference or two).
  
#Rules of Thumb for Experimental Data

More to come so watch this space. Very generic rules of thumb:  

- To say *anything* about the mean, make at least $n = 6$ measurements and preferably $n=10$: the reason for 6 is that this is the minimum number of observations necessary to determine whether something is statistically significant using non-parametric analysis. Joe Howard (ref.) gives the following example: 'If one tosses a (fair) coin 6 times, then there is a 1/64 chance that it is heads each time and a 1/64 chance that it is tails each time. Therefore, if you toss a coin 6 times there is only a 1/32 chance (~3%) that it will always be heads or always tails. In other words, if you toss a coin 6 times and it is always heads or tails then you can conclude at the 95% confidence level (actually 97%) that the coin is not fair. Note that the 3% (1/32) is the chance of have a fair coin and getting a false positive, meaning a result that appears to contradict reality';
- To say something about the variance, have at least $n =30$ data points;
- To say anything about the actual distribution, have at least $n = 100$ data points.