---
title: "Notes on practical statistics I"
output:
  html_document:
    toc: true
    fig_caption: true
    number_sections: true
---


#An Incomplete Compendium of Distributions

This section is a prototype: I need to flesh out equations, descriptions, motivaations and whatever else (eg. means, variances etc...). Oh, and include Figures: make it at least a little bit pleasing on the eye, please, Hugo!

##Normal (Gaussian) Distribution

Motivation:

- The Central Limit Theorem! WOWSER.
- Things in the phenomenal world are often normally distributed (this is why the term 'bell curve' is part of our general society's lexicon, rather than merely a technical term);
- Measurement error (e.g. using a ruler) is often normally distributed.

Equation: $P(x) = \frac{1}{\sigma\sqrt{2\pi}}\text{exp}(-\frac{(x - \mu)^2}{2\sigma^2})$

##Binomial Distribution

Motivation:

- Flipping coins!

Equation: $P(k|n) = {n\choose k}p^k(1 - p)^{n-k}$, where $p$ is the binomial probability (the probability of flipping heads).

##Poisson Distribution

Motivation:

- Counting things.

Equation: $P(k) = \frac{\lambda^k}{k!}e^{-\lambda}$, for $k \in \{0,1,2,3,\ldots\}$.

##Exponential Distribution

Motivation:

- Radioactive decay;
- Any Poisson Process has exponentially distributed waiting times (will prove below OR in appendix; will included numerical example in the exercises);

Equation: $P(x) = \frac{\text{exp}(-x/\mu)}{\mu}$.

##Gamma Distribution

Motivation:

- Results from multi-step processes (and thus generalizes the exponential distribution)

Equation: $P(x) = \frac{\beta^{\alpha}}{\Gamma(\alpha)}x^{\alpha - 1}e^{-\beta x}$.

##Other Distributions

This list is already long enough for the current purposes of introduction to distributions. There are also Pareto (power-law) distributions, Cauchy distributions, Laplace distributions and a very many more... (perhaps list more with a reference or two).

#Statistical Hypothesis Testing
  
##Definition of the F-statistic

The following is primarily from Rice, Ch. 12: let's say that we have $I$ groups, each with $J$ samples, and let $Y_{ij}=$ the $j$ observation in the $i$th group. The statistical model is

$$Y_{ij} = \mu + \alpha_{i} + \varepsilon_{ij}.$$
Assumption: $\varepsilon_{ij}$ are i.i.d Gaussian; we also normalize such that $\sum\alpha_i = 0$.
Then
$$SS_{TOT} = SS_{W} + SS_{B},$$

where $SS_{TOT} = \sum\limits_{ij}(Y_{ij}-\bar{Y}_{..})^2$ is the total sum of squares of the data, $SS_{W}=\sum\limits_{ij}(Y_{ij}-\bar{Y}_{i.})^2$ is the sum of squares within groups (unexplained variance) & $SS_{B}=\sum\limits_{i}(\bar{Y}_{i.}-\bar{Y}_{..})^2$ is the sum of squares between groups (explained variance). We look at the F-statistic

$$F = \frac{SS_{B}/(I-1)}{SS_{W}/I(J-1)}$$

and use it to test the null hypothesis $H_0: \alpha_1 = \alpha_2 = \ldots = \alpha_I = 0,$ which will be distributed with $(I-1)$ and $I(J-1)$ degrees of freedom.

Problem: if the null hypothesis is rejected, we do not know which $\alpha_i$'s are different from 0.
