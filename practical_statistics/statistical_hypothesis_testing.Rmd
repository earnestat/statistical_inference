---
title: "Notes on practical statistics II"
output:
  html_document:
    toc: true
    fig_caption: true
    number_sections: true
---

#Statistical Hypothesis Testing: An Intuition Towards

Two key aspects of practical stastics are:

- statistical hypothesis testing;
- model fitting (parameter estimation) & model selection.

In this note, we will deal with the first aspect, statistical hypothesis testing.

First: what is an hypothesis? Five examples are:

- The speed of light in a vacuum is 299,792,458 m/s;
- The mean height of adult men in Australia is 175cm;
- The average speed of vesicles along microtubules (in *S. pombe*) is $0.5 \mu m/s$;
- Drug A improves the sleep quality of people who suffer from insomia;
- An advertisement at the top of my website generates more clicks than one at the bottom of my website.

Definition (Oxford Dictionary, online): an hypothesis "[a] supposition or proposed explanation made on the basis of limited evidence as a starting point for further investigation."

Now: what is statistial hypothesis testing? It is merely testing how likely a hypothesis is, given the data at hand.

#Essential question: is my data normally distributed?

Some statisticians and physicists would baulk at this question (I've seen it first-hand, in fact). They would tell you (and they would be correct, in all their pedantry) that no data is normally distributed, but some is so close to being normally distributed that it is not statistically different to a normal distribution.

So the question becomes: is the data \~ normally distributed? A robust, quantitative way of thinking about this is to perform one or more of a number of hypothesis tests, which we'll get to, but first, let's consider it visually with the aid of Q-Q plots. To check out Q-Q plots, we need to define a *quantile*:

Definition: a *quantile* of a dataset is the value below which a given percent (or fraction) of the data are: for example, the 10% quantile (also known as the *first decile*) is the value below which 10% of the data are; similarly, the 25% quantile (or the *first quartile* $Q_1$), the 50% quantile (the *second quartile* $Q_2$ or the *median*) and the 75% quantile (or the *third quartile* $Q_3$) are the values below which 1/4, 1/2 and 3/4 of the data are, respectively.

Now all that a Q-Q (quantile-quantile) plot is is a plot of the quantiles of one set of data against the quantiles of another. Below are two examples. The first is the (quantiles of) data generated from a normal distribution plotted against (quantiles of) a theoretic normal distribution. In the second, the data is from a Caunchy distribution (symmetric and with heavier tails than a Gaussian), the theoretic a normal distribution.

```{r, fig.width = 8 , fig.height = 3 , message = FALSE , echo=FALSE}
#code chunk here:
set.seed( 42 )
library( ggplot2 )
source("multiplot.R")
qqplot.data <- function (vec) # argument: vector of numbers
{
  # following four lines from base R's qqline()
  y <- quantile(vec[!is.na(vec)], c(0.25, 0.75))
  x <- qnorm(c(0.25, 0.75))
  slope <- diff(y)/diff(x)
  int <- y[1L] - slope * x[1L]

  d <- data.frame(resids = vec)

  ggplot(d, aes(sample = resids)) + stat_qq() + geom_abline(slope = slope, intercept = int,
                                                            color = "red")
}
n <- 500
x <- rnorm( n )
y <- rcauchy( n )
z <- rnorm(100)
p1 <- qqplot.data( x ) + xlab("theoretical (Gaussian)") + ylab("sample from Gaussian")
p2 <- qqplot.data( y ) + xlab("theoretical (Gaussian)") + ylab("sample from Cauchy")
multiplot(p1, p2, cols=2)
```

- You can distinctly see the heavier tail of the Cauchy distribution here!
- You can also plot histograms to test the normality hypothesis by eye.

<h4>Lilliefors Test (for normality)</h4>

The Lilliefors test for normality is a particular case of the Kolmogorov-Smirnov test (we won't cover this here but do check it out because it is one of the great statistical tests!). Given a dataset $D =\{ x_i \}$, we wish to test the *null hypothesis* $H_0,$ which states that $D$ is drawn from a normal distribution.

1. We compare the cumulative distribution functions of $D$ and the normal distribution with mean $\mu_X$ & variance $\sigma_X^2$. To do so we calculate the largest distance $D_{max}$ between them;
2. If the null hypothesis is true, then $D_{max}$ will be drawn from a **Lilliefors distribution** (the distribution itself will depend on the sample size $n$; there are tables, along with more recent analytic approximations, for those interested: http://www.jstor.org/stable/2684607);
3. Using this Lilliefors distribution, we calculate $p$, the probability of seeing $D_{max}$, given the null hypothesis $H_0$, that is, the assumption of normality. Classically, $p$ has been called the $p$-value.
4. If $p<\alpha,$ we can conclude with $(1-\alpha)$ confidence that $D$ was not drawn from a normal distribution. Classically, it is required that $\alpha = 0.05$, in which case we can conclude with 95% confidence that $D$ was not drawn from a normal distribution.

Here we plot the empirical CDFs of a Cauchy and a Gaussian distribution: can you tell which is which? I should probably plot each separately against a theoretical Gaussian so that we can visually make sense of the distance $D_{max}$.
```{r , message=FALSE , echo=FALSE}
df <- data.frame(x = c(rnorm( 500 , mean = mean(y) , sd = sd(y)), y),
                 g = gl(2, 500))

ggplot(df, aes(x, colour = g)) + stat_ecdf() #+ xlim(c(-15,15))
```


```{r , message=FALSE , echo=TRUE}
#http://www.inside-r.org/packages/cran/nortest/docs/lillie.test
library(nortest)
lillie.test(x); lillie.test(y)
```

Thus, as the $p$-value for $y$ is less than $10^{-3}$, we can conclude with $>99.9$% confidence that $y$ was NOT drawn from a normal distribution (and phew! as we generated $y$ from a Cauchy distribution). Moreover, as the $p$-value for $x$ was $>0.2$, we cannot conclude with any credible confidence that it was not drawn from a normal distribution (this is also a sanity check).

for more cool approaches (thinking about skewness, kurtosis; other statistical tests, such as Shapiro-Walk and Anderson-Darling, see here: http://statsthewayilikeit.com/about/is-my-data-normally-distributed/)

#Student's t-tests

##One sample t-test

I am measuring a quantity in an experiment (e.g., a mutant) for which there is a control. This quantity has a specific value $\mu$ and my measurement process has a normal error (e.g., amount of protein using a fluorospectrometer). Let $x_1,x_2,\ldots,x_n$ be $n$ independent samples: this means that they will be drawn from the distribution $N( \mu , \sigma^2)$. We want to answer the question: is the amount $\mu$ in the (mutant) experiment different from that in the control $\mu_0$? To answer this, the null hypothesis is
$$H_0: \mu = \mu_0.$$
Then the alternative hypothesis is
$$H_1: \mu \neq \mu_0.$$
Here we beeswarm plot some normally distributed data, along with a horizontal line to represent a possible null hyothesis (here $\mu_0 = -0.7$ is represented by a horizontal red line).

```{r , message=FALSE , echo=FALSE , include=FALSE}
library(beeswarm)
bs1 <- beeswarm(rnorm(250),
         pch = 16 , do.plot = TRUE)
bs1$type <- "mutant"
```

```{r , message=FALSE , echo=FALSE }
dfl <- as.data.frame(cbind(seq(min(bs1$x),max(bs1$x) , by = max(bs1$x) - min(bs1$x)), rep(-0.7,2)))
ggplot( bs1 , aes( x = x , y =y  )) + geom_point(size = 3) +
  xlab("") + ylab("value")  +
  scale_x_continuous(breaks = c(1), 
                    labels = c("data") ) + geom_line(data = dfl , aes(x = V1,y = V2 , color = "red",
                                                                     size = 10)) + 
      guides(color=FALSE , size = FALSE)
```

To test the null hypothesis, we consider the t-statistic
$$T = \frac{|\bar{x}-\mu_0|}{\text{SEM}}.$$
Intuition: this t-statistic measures how many "standard errors" the sample mean is away from the (null) hypothesized mean.

Formalism: this statistic has the Student t-distribution $\text{Pr}(t)$ with $n-1$ degrees of freedom. The p-value is defined as the area under the curve where $|t|>|T|$ (include relevant plot here).

Example: if the p-value $<0.05$, we conclude with $95\%$ confidence that $\mu \neq \mu_0$. In general, if the p-value $<\alpha$, then we can conclude with $100(1-\alpha)\%$ confidence that $\mu\neq\mu_0$.

Question: Given $n=16$ data points, what is the minimum effect size $|\bar{x} - \mu_0|$ that you can detect at the $95\%$ confidence level? 

Answer: at the $95\%$ confidence level, we require that $T>2$ (actually a bit more); but 
$T =\frac{|\bar{x}-\mu_0|\sqrt{n}}{\sigma}$. Combining these and plugging in $n = 16$, we see that $|x - \mu_0| > \sigma/2$. In other words, with 16 data points, we can only detect effect sizes greater than half the standard deviation of the measurement error at the $95\%$ confidence level! 


##Welch's t-test

We want to see whether or not there is a significant difference between the means of 2 sets of measurements $x_1 , x_2 , \ldots , x_n$ (control) & $y_1 , y_2 , \ldots , y_m$ (experiment).

```{r , message=FALSE , echo=FALSE , include=FALSE }
bs1 <- beeswarm(rnorm(250),
         pch = 16 )
bs2 <- beeswarm(rnorm(250,1),
                pch = 16 ) 
bs1$type <- "control"
bs2$type <- "mutant"
bs2$x <- bs2$x +0.4
bs <- rbind(bs1 , bs2)
```

```{r , message=FALSE , echo=FALSE }
ggplot( bs , aes( x = x , y =y , color = type )) + geom_point(size=3) +
  xlab("data") + ylab("value") 
  scale_x_continuous(breaks = c(1:2), 
                    labels = c("control" , "mutant"), expand = c(0, 0.5)) +
  theme(axis.title=element_text(size=22) ,
        axis.text=element_text(size=22))
```
The t-statistic we use in this case is
$$T = \frac{|\bar{x} - \bar{y}|}{\sqrt{\text{SEM}_x^2 + \text{SEM}_y^2}}$$
and the degrees of freedom $\nu$ is approximated by the *Welch-Satterthwaite equation*:
$$\nu \approx \frac{(\text{SEM}_x^2 + \text{SEM}_y^2)^2}{\text{SEM}_x^4/(n-1) + 
\text{SEM}_y^4/(n-1)}$$

See Welch, B. L. (1947), "The generalization of "student's" problem when several different population variances are involved.", Biometrika 34: 28–35.  & https://en.wikipedia.org/wiki/Welch%27s_t_test (any easy reference: text book? Hastie?)

N.b. If the population variances are same & so are the sample sizes, then we can do better!

##Two sample t-test

The two-sample t-test assumes that the number of measurements are the same (i.e., $n=m$) and that the two distributions have the same variance  (i.e., $\text{SD}_x = \text{SD}_y$). In this case, we can use all the measurements to estimate the underlying population variance such that $\text{SEM} = \sqrt{\text{SEM}_x^2 + \text{SEM}_y^2}/2$ and thus
$$T = \frac{2|\bar{x} - \bar{y}|}{\sqrt{\text{SEM}_x^2 + \text{SEM}_y^2}}.$$

To quote Joe Howard's *Statistical Inference and Models* notes for MB&B 435/635/ENAS518 - MATHEMATICAL METHODS IN BIOPHYSICS at Yale University, Fall 2013)

<blockquote><p style="font-size: 75%">Importantly, it could happen that the difference is significant by the unpaired t-test but not significant by the Welch test. This illustrates the general principle that the more assumption you make the more significant your results will be! The choice as to which one to use depends on how confident we are that the SDs are the same (i.e. how strong your expectation is). But generally it is better to make the minimum number of assumptions, and this is why the Welch test is preferred. Also, there are many cases where the variances of the two experiments are different - e.g. if you are measuring something using two different techniques.</p></blockquote>

```{r , message=FALSE , echo=TRUE }
data <- read.csv("MG_catastrophe_data/lifetimes_tubulin.csv" , check.names=FALSE) #load data from .csv
mlt <- apply( data , 2 , function(x) mean(x,na.rm = TRUE))
n <- apply( data , 2 , function(x) sum(!is.na(x))) #number of data points
sem <- mlt/sqrt(n) #standard errors
dfnew <- data.frame(cbind(colnames(data),as.numeric(mlt),as.numeric(sem)))
p <- ggplot(dfnew, aes( x = X1 , y = mlt))
p + geom_point() +
  geom_errorbar(aes(ymin=mlt-sem, ymax=mlt+sem), width=.1)
```

```{r , message=FALSE , echo=TRUE }
t.test( subset(data$`9_uM` , !is.na(data$`9_uM`) ) , subset(data$`7_uM` , !is.na(data$`7_uM`) ) )
t.test( subset(data$`12_uM` , !is.na(data$`12_uM`) ) , subset(data$`10_uM` , !is.na(data$`10_uM`) ) )
```

##Paired t-tests and Blocking
<span style="color:red">Perhaps motivate this at the end of the previous section?</span>

<h4>Paired t-test</h4>

In the wise words of Joe Howard, **"I recommend, whenever possible, designing experiments so you can use a paired t-test."** Why would he say this?

Well, a two-sample paired t-test is a regular two-sample t-test in which each $x_i$ (control) has a corresponding $y_i$ (experiment). The easiest way to achieve this is by performing the control and experiment together a number of times so that all conditions are the same for each $x_i$ and $y_i$ (besides the one variable corresponding to the experiment, e.g., a genetic knockdown). The you just use the one-sample t-test on the differences $\{x_i - y_i\}$ with the null hypothesis that $\mu_{X-Y}=0$: BOOM!

Example: Provide an example (with a Figure).

<h4>Blocking & Randomized Block Design</h4>

Intuition: Let's say that you do a bunch of controls and experiments under slightly different conditions (e.g. different days) -- then you want to 'block' together those done under similar conditions. More to come. Watch this space.



#Correction Methods (E.g. the Bonferroni Correction)

Problem: When you are performing and comparingmany experiments, there is a problem. For example, let's take the absurd case in which you perform an experiment that has no effect 20 times. We know that 5% of the time you will get a statistically significant difference at the 95% confidence level (due to statistical fluctuations): this is one in 20 of your experiments! We then need a stricter criterion for statistical significance: enter the Bonferroni Correction.

The Bonferroni Correction: let's say that you perform $n$ experiments and you want a confidence level of $1-\alpha$ ( 95% confidence means $\alpha = 0.05$). We reject the null hypothesis at the $1-\alpha$ confidence level if $p<\alpha/N$ (as opposed to if $p<\alpha$, the uncorrected version of tht t-test).

Example: Give an example here. Make a figure. See https://stat.ethz.ch/R-manual/R-patched/library/stats/html/p.adjust.html for implementation.

Note: this is a conservative test, in the sense that it is prone to producing false negatives (*type II errors*).


#F-test
<span style="color:red">The headings may need to be organized slightly differently.</span>

Give the general idea here.

##Example 1: ANOVA (analysis of variance).

<h4>Distinguishing multiple groups of numerical data with distinct means.</h4>

The following is primarily from Rice, Ch. 12: let's say that we have $I$ groups, each with $J$ samples, and let $Y_{ij}=$ the $j$ observation in the $i$th group. The statistical model is

$$Y_{ij} = \mu + \alpha_{i} + \varepsilon_{ij}.$$
Assumption: $\varepsilon_{ij}$ are i.i.d Gaussian; we also normalize such that $\sum\alpha_i = 0$.
Then
$$SS_{TOT} = SS_{W} + SS_{B},$$

where $SS_{TOT} = \sum\limits_{ij}(Y_{ij}-\bar{Y}_{..})^2$ is the total sum of squares of the data, $SS_{W}=\sum\limits_{ij}(Y_{ij}-\bar{Y}_{i.})^2$ is the sum of squares within groups (unexplained variance) & $SS_{B}=\sum\limits_{i}(\bar{Y}_{i.}-\bar{Y}_{..})^2$ is the sum of squares between groups (explained variance). We look at the F-statistic

$$F = \frac{SS_{B}/(I-1)}{SS_{W}/I(J-1)}$$

and use it to test the null hypothesis $H_0: \alpha_1 = \alpha_2 = \ldots = \alpha_I = 0,$ which will be distributed with $(I-1)$ and $I(J-1)$ degrees of freedom.

Problem: if the null hypothesis is rejected, we do not know which $\alpha_i$'s are different from 0.

```{r, fig.width = 8 , fig.height = 6 , message = FALSE , echo=FALSE}
#code chunk here:
library(beeswarm)
library( ggplot2 )
setwd("~/repos/statistical_inference/practical_statistics/")
#data from here:
#http://www.stat.ufl.edu/~winner/datasets.html
data <- read.table("agedeath.dat", header=FALSE)
colnames(data) <- c('type','age','number')
data$x = as.numeric(data$type)
beeswarm <- beeswarm( age ~ type , data = data  , pch =16, pwcol = type,
                      method = 'swarm' , do.plot = FALSE)[, c(1, 2, 4, 6)]
beeswarm$x <- beeswarm$x + 1*(as.numeric(beeswarm$col)-1)
beeswarm.plot <- ggplot(beeswarm, aes(x, y) , color = col ) +
  xlab("") + ylab("age at death")
  scale_y_continuous(expression("age") , expand = c(0, 3)) 
beeswarm.plot + geom_point(size = 2 , aes(colour = col))
```

##Example 2: testing for multimodality.
```{r , fig.width = 4 , fig.height = 3 , message = FALSE , echo=FALSE}
##Galton height data: http://personality-project.org/r/html/heights.html
##Galton paper here: http://galton.org/essays/1880-1889/galton-1888-co-relations-royal-soc/galton_corr.html
x <- rnorm( 350 , mean = 0)
y <- rnorm( 350 , mean = 2.5 )
d <- as.data.frame( c( x , y) )
colnames(d) <- 'value'
p2 <- qplot(d$value) + xlab('value')
p2
```

Is the population from which I have drawn the above data $X$ unimodal or multimodal? It may look bimodal, however an apparent 2nd peak can appear due to statistical fluctuaitons and sampling error. This question is very important because the answer could, for example, tell us that we actually have two distinct sub-populations.

Algorithm:

1. Bin the data;
2. Compute tha variance $Var_X = Var(X)$ of the data;
3. For each bin (indexed by $i$), divide the data into two sub-population $X_{i1},X_{i2}$, separated by the end point of the bin. Compute the mean variance $Var_i = (Var(X_{i1}) + Var(X_{i2}))/2$;
4. Pick the decomposition of $X$ that minimizes $Var_i$;
5. Compute the F-statistic $F=\text{min}(Var_i)/Var_i$;
6. Compute the p-value, given $F$ and $(1,n+m-2)$ degrees of freedom, where $n$ and $m$ are the respective sizes of the sub-populations in the decomposotion that minimizes $Var_i$.

Examples: Give a few examples.


For more, see Larkin R. P. (1979). An algorithm for assessing bimodality vs. unimodality in a univariate distribution. Behav. Res. Methods Instrum. 11, 467–468.
